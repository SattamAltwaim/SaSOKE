{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SattamAltwaim/SaSOKE/blob/DHM-tech/notebooks/3_train_soke(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1UWuSWR1uCJ"
      },
      "source": [
        "# SOKE Stage 2: Train Sign Language Generator\n",
        "Trains the mBART-based multilingual sign language generator using tokenized poses.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WBja77SQ1uCL",
        "outputId": "49045e61-3cd0-4c80-9c95-4c6f66fed9b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SaSOKE'...\n",
            "remote: Enumerating objects: 506, done.\u001b[K\n",
            "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 506 (delta 4), reused 0 (delta 0), pack-reused 495 (from 1)\u001b[K\n",
            "Receiving objects: 100% (506/506), 2.47 MiB | 13.99 MiB/s, done.\n",
            "Resolving deltas: 100% (219/219), done.\n",
            "/content/SaSOKE\n",
            "Branch 'DHM-tech' set up to track remote branch 'DHM-tech' from 'origin'.\n",
            "Switched to a new branch 'DHM-tech'\n",
            "Collecting pytorch-lightning>=2.0.0 (from -r requirements_colab.txt (line 3))\n",
            "  Downloading pytorch_lightning-2.5.6-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting torchmetrics>=1.0.0 (from -r requirements_colab.txt (line 4))\n",
            "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: transformers>=4.30.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 7)) (4.57.2)\n",
            "Requirement already satisfied: sentencepiece>=0.1.99 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 8)) (0.2.1)\n",
            "Requirement already satisfied: diffusers>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 9)) (0.35.2)\n",
            "Requirement already satisfied: omegaconf>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 12)) (2.3.0)\n",
            "Requirement already satisfied: pyyaml>=6.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 13)) (6.0.3)\n",
            "Collecting shortuuid>=1.0.0 (from -r requirements_colab.txt (line 16))\n",
            "  Downloading shortuuid-1.0.13-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 17)) (0.8.1)\n",
            "Requirement already satisfied: more-itertools>=10.0.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 18)) (10.8.0)\n",
            "Requirement already satisfied: natsort>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 19)) (8.4.0)\n",
            "Collecting ftfy>=6.1.0 (from -r requirements_colab.txt (line 20))\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: wandb>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 23)) (0.23.0)\n",
            "Requirement already satisfied: tensorboard>=2.14.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 24)) (2.19.0)\n",
            "Requirement already satisfied: rich>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 25)) (13.9.4)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 26)) (3.10.0)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 29)) (2.2.2)\n",
            "Requirement already satisfied: h5py>=3.9.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 30)) (3.15.1)\n",
            "Requirement already satisfied: scikit-image>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 31)) (0.25.2)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 32)) (4.67.1)\n",
            "Collecting smplx>=0.1.28 (from -r requirements_colab.txt (line 35))\n",
            "  Downloading smplx-0.1.28-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting trimesh>=3.22.0 (from -r requirements_colab.txt (line 36))\n",
            "  Downloading trimesh-4.10.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: spacy>=3.6.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 39)) (3.8.11)\n",
            "Requirement already satisfied: gdown>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 42)) (5.2.0)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning>=2.0.0->-r requirements_colab.txt (line 3)) (2.9.0+cu126)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning>=2.0.0->-r requirements_colab.txt (line 3)) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning>=2.0.0->-r requirements_colab.txt (line 3)) (25.0)\n",
            "Requirement already satisfied: typing-extensions>4.5.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning>=2.0.0->-r requirements_colab.txt (line 3)) (4.15.0)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch-lightning>=2.0.0->-r requirements_colab.txt (line 3))\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics>=1.0.0->-r requirements_colab.txt (line 4)) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers>=4.30.0->-r requirements_colab.txt (line 7)) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.30.0->-r requirements_colab.txt (line 7)) (0.36.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.30.0->-r requirements_colab.txt (line 7)) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers>=4.30.0->-r requirements_colab.txt (line 7)) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.30.0->-r requirements_colab.txt (line 7)) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.30.0->-r requirements_colab.txt (line 7)) (0.7.0)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from diffusers>=0.20.0->-r requirements_colab.txt (line 9)) (8.7.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from diffusers>=0.20.0->-r requirements_colab.txt (line 9)) (11.3.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from omegaconf>=2.3.0->-r requirements_colab.txt (line 12)) (4.9.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy>=6.1.0->-r requirements_colab.txt (line 20)) (0.2.14)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.15.0->-r requirements_colab.txt (line 23)) (8.3.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.15.0->-r requirements_colab.txt (line 23)) (3.1.45)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb>=0.15.0->-r requirements_colab.txt (line 23)) (4.5.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.15.0->-r requirements_colab.txt (line 23)) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.15.0->-r requirements_colab.txt (line 23)) (2.12.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.15.0->-r requirements_colab.txt (line 23)) (2.46.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.14.0->-r requirements_colab.txt (line 24)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.14.0->-r requirements_colab.txt (line 24)) (1.76.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.14.0->-r requirements_colab.txt (line 24)) (3.10)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.14.0->-r requirements_colab.txt (line 24)) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.14.0->-r requirements_colab.txt (line 24)) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.14.0->-r requirements_colab.txt (line 24)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.14.0->-r requirements_colab.txt (line 24)) (3.1.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.0.0->-r requirements_colab.txt (line 25)) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.0.0->-r requirements_colab.txt (line 25)) (2.19.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->-r requirements_colab.txt (line 26)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->-r requirements_colab.txt (line 26)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->-r requirements_colab.txt (line 26)) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->-r requirements_colab.txt (line 26)) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->-r requirements_colab.txt (line 26)) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->-r requirements_colab.txt (line 26)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->-r requirements_colab.txt (line 29)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->-r requirements_colab.txt (line 29)) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.21.0->-r requirements_colab.txt (line 31)) (1.16.3)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.21.0->-r requirements_colab.txt (line 31)) (3.6)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.21.0->-r requirements_colab.txt (line 31)) (2.37.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.21.0->-r requirements_colab.txt (line 31)) (2025.10.16)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.21.0->-r requirements_colab.txt (line 31)) (0.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.6.0->-r requirements_colab.txt (line 39)) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.6.0->-r requirements_colab.txt (line 39)) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.6.0->-r requirements_colab.txt (line 39)) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.6.0->-r requirements_colab.txt (line 39)) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.6.0->-r requirements_colab.txt (line 39)) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.6.0->-r requirements_colab.txt (line 39)) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.6.0->-r requirements_colab.txt (line 39)) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.6.0->-r requirements_colab.txt (line 39)) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.6.0->-r requirements_colab.txt (line 39)) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.6.0->-r requirements_colab.txt (line 39)) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.6.0->-r requirements_colab.txt (line 39)) (0.20.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.6.0->-r requirements_colab.txt (line 39)) (3.1.6)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown>=4.7.0->-r requirements_colab.txt (line 42)) (4.13.5)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning>=2.0.0->-r requirements_colab.txt (line 3)) (3.13.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.15.0->-r requirements_colab.txt (line 23)) (4.0.12)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.30.0->-r requirements_colab.txt (line 7)) (1.2.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=13.0.0->-r requirements_colab.txt (line 25)) (0.1.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb>=0.15.0->-r requirements_colab.txt (line 23)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb>=0.15.0->-r requirements_colab.txt (line 23)) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb>=0.15.0->-r requirements_colab.txt (line 23)) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.30.0->-r requirements_colab.txt (line 7)) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.30.0->-r requirements_colab.txt (line 7)) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.30.0->-r requirements_colab.txt (line 7)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.30.0->-r requirements_colab.txt (line 7)) (2025.11.12)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy>=3.6.0->-r requirements_colab.txt (line 39)) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy>=3.6.0->-r requirements_colab.txt (line 39)) (0.1.5)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning>=2.0.0->-r requirements_colab.txt (line 3)) (1.14.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning>=2.0.0->-r requirements_colab.txt (line 3)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning>=2.0.0->-r requirements_colab.txt (line 3)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning>=2.0.0->-r requirements_colab.txt (line 3)) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning>=2.0.0->-r requirements_colab.txt (line 3)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning>=2.0.0->-r requirements_colab.txt (line 3)) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning>=2.0.0->-r requirements_colab.txt (line 3)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning>=2.0.0->-r requirements_colab.txt (line 3)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning>=2.0.0->-r requirements_colab.txt (line 3)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning>=2.0.0->-r requirements_colab.txt (line 3)) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning>=2.0.0->-r requirements_colab.txt (line 3)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning>=2.0.0->-r requirements_colab.txt (line 3)) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning>=2.0.0->-r requirements_colab.txt (line 3)) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning>=2.0.0->-r requirements_colab.txt (line 3)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning>=2.0.0->-r requirements_colab.txt (line 3)) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning>=2.0.0->-r requirements_colab.txt (line 3)) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning>=2.0.0->-r requirements_colab.txt (line 3)) (3.5.0)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy>=3.6.0->-r requirements_colab.txt (line 39)) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy>=3.6.0->-r requirements_colab.txt (line 39)) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.14.0->-r requirements_colab.txt (line 24)) (3.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown>=4.7.0->-r requirements_colab.txt (line 42)) (2.8)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->diffusers>=0.20.0->-r requirements_colab.txt (line 9)) (3.23.0)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown>=4.7.0->-r requirements_colab.txt (line 42)) (1.7.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=2.0.0->-r requirements_colab.txt (line 3)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=2.0.0->-r requirements_colab.txt (line 3)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=2.0.0->-r requirements_colab.txt (line 3)) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=2.0.0->-r requirements_colab.txt (line 3)) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=2.0.0->-r requirements_colab.txt (line 3)) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=2.0.0->-r requirements_colab.txt (line 3)) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=2.0.0->-r requirements_colab.txt (line 3)) (1.22.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.15.0->-r requirements_colab.txt (line 23)) (5.0.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy>=3.6.0->-r requirements_colab.txt (line 39)) (2.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.1.0->pytorch-lightning>=2.0.0->-r requirements_colab.txt (line 3)) (1.3.0)\n",
            "Downloading pytorch_lightning-2.5.6-py3-none-any.whl (831 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m831.6/831.6 kB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shortuuid-1.0.13-py3-none-any.whl (10 kB)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smplx-0.1.28-py3-none-any.whl (29 kB)\n",
            "Downloading trimesh-4.10.0-py3-none-any.whl (736 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m736.6/736.6 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: trimesh, shortuuid, lightning-utilities, ftfy, torchmetrics, smplx, pytorch-lightning\n",
            "Successfully installed ftfy-6.3.1 lightning-utilities-0.15.2 pytorch-lightning-2.5.6 shortuuid-1.0.13 smplx-0.1.28 torchmetrics-1.8.2 trimesh-4.10.0\n",
            "Mounted at /content/drive\n",
            "✓ Setup complete!\n",
            "Code: /content/SaSOKE\n",
            "Data: /content/drive/MyDrive/GraduationProject/CodeFiles/SaSOKE\n"
          ]
        }
      ],
      "source": [
        "# Clone repo if not present\n",
        "import os\n",
        "if not os.path.exists('/content/SaSOKE'):\n",
        "    !git clone https://github.com/SattamAltwaim/SaSOKE.git\n",
        "    %cd /content/SaSOKE\n",
        "    !git checkout DHM-tech\n",
        "else:\n",
        "    %cd /content/SaSOKE\n",
        "    !git checkout DHM-tech\n",
        "    !git pull origin DHM-tech\n",
        "\n",
        "# Install dependencies\n",
        "!pip install -r requirements_colab.txt\n",
        "\n",
        "# Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "drive_data = '/content/drive/MyDrive/GraduationProject/CodeFiles/SaSOKE'\n",
        "\n",
        "# Link dependencies from Drive\n",
        "!rm -rf deps checkpoints smpl-x\n",
        "!ln -sf {drive_data}/deps deps\n",
        "!ln -sf {drive_data}/smpl-x smpl-x\n",
        "!ln -sf {drive_data}/checkpoints checkpoints\n",
        "\n",
        "print(\"✓ Setup complete!\")\n",
        "print(\"Code:\", os.getcwd())\n",
        "print(\"Data:\", drive_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fIbdJ2G1uCM"
      },
      "source": [
        "## Prerequisites\n",
        "Ensure tokenizer is trained or pretrained checkpoint exists at `checkpoints/vae/tokenizer.ckpt`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "J0m4wk8d1uCN",
        "outputId": "407550a6-42a9-4ec1-8c0a-2be0dae55acd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer checkpoint found in Drive\n"
          ]
        }
      ],
      "source": [
        "# Verify tokenizer checkpoint in Drive\n",
        "assert os.path.exists(f'{drive_data}/checkpoints/vae/tokenizer.ckpt'), \"Tokenizer not found in Drive!\"\n",
        "print(\"Tokenizer checkpoint found in Drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlpxDzlV1uCN"
      },
      "source": [
        "## Configuration Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mnm2t2vX1uCN",
        "outputId": "a4d5b4cf-2d30-4aad-df01-556670d16df5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Config updated - GitHub code + Drive data\n"
          ]
        }
      ],
      "source": [
        "# Update config for Colab/CUDA\n",
        "import yaml\n",
        "\n",
        "with open('configs/soke.yaml', 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "# GPU settings\n",
        "config['ACCELERATOR'] = 'gpu'\n",
        "config['DEVICE'] = [0]\n",
        "\n",
        "# Point to Drive for data/models\n",
        "config['DATASET']['H2S']['ROOT'] = f'{drive_data}/data/How2Sign'\n",
        "config['DATASET']['H2S']['MEAN_PATH'] = f'{drive_data}/smpl-x/mean.pt'\n",
        "config['DATASET']['H2S']['STD_PATH'] = f'{drive_data}/smpl-x/std.pt'\n",
        "\n",
        "# Model paths in Drive\n",
        "config['TRAIN']['PRETRAINED_VAE'] = f'{drive_data}/checkpoints/vae/tokenizer.ckpt'\n",
        "config['model']['params']['lm_path'] = f'{drive_data}/deps/mbart-h2s-csl-phoenix'\n",
        "\n",
        "# Training settings\n",
        "config['TRAIN']['NUM_WORKERS'] = 2\n",
        "config['TRAIN']['BATCH_SIZE'] = 16\n",
        "\n",
        "# Save config\n",
        "with open('configs/soke_colab.yaml', 'w') as f:\n",
        "    yaml.dump(config, f)\n",
        "\n",
        "print(\"Config updated - GitHub code + Drive data\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CRITICAL: Generate GloVe Word Embeddings\n",
        "# ============================================\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pickle\n",
        "from os.path import join as pjoin\n",
        "\n",
        "glove_dir = \"/content/drive/MyDrive/GraduationProject/CodeFiles/SaSOKE/deps/t2m/glove/\"\n",
        "os.makedirs(glove_dir, exist_ok=True)\n",
        "\n",
        "print(\"Step 1: Downloading GloVe embeddings...\")\n",
        "# Download GloVe 6B.300d\n",
        "!wget -q --show-progress http://nlp.stanford.edu/data/glove.6B.zip -O /content/glove.6B.zip\n",
        "\n",
        "print(\"\\nStep 2: Extracting...\")\n",
        "import zipfile\n",
        "with zipfile.ZipFile(\"/content/glove.6B.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extract(\"glove.6B.300d.txt\", \"/content/\")\n",
        "\n",
        "print(\"\\nStep 3: Preparing vocabulary for How2Sign dataset...\")\n",
        "\n",
        "# Read the text annotations from How2Sign to build vocabulary\n",
        "# This creates the word-to-index mapping\n",
        "from collections import Counter\n",
        "import json\n",
        "\n",
        "# Get vocabulary from your How2Sign dataset\n",
        "h2s_root = f\"{drive_data}/data/How2Sign\"\n",
        "vocab_words = set()\n",
        "\n",
        "# Scan through your dataset to collect all words\n",
        "for split in ['train', 'val', 'test']:\n",
        "    split_file = f\"{h2s_root}/how2sign_{split}.txt\"\n",
        "    if os.path.exists(split_file):\n",
        "        with open(split_file, 'r') as f:\n",
        "            for line in f:\n",
        "                words = line.strip().lower().split()\n",
        "                vocab_words.update(words)\n",
        "        print(f\"  Loaded {split} vocabulary\")\n",
        "\n",
        "# Add special tokens\n",
        "special_tokens = ['<pad>', '<unk>', '<sos>', '<eos>']\n",
        "vocab_words.update(special_tokens)\n",
        "\n",
        "print(f\"\\nTotal vocabulary size: {len(vocab_words)}\")\n",
        "\n",
        "# Load GloVe embeddings\n",
        "print(\"\\nStep 4: Loading GloVe vectors...\")\n",
        "glove_file = \"/content/glove.6B.300d.txt\"\n",
        "glove_embeddings = {}\n",
        "\n",
        "with open(glove_file, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        if word in vocab_words:\n",
        "            vector = np.asarray(values[1:], dtype='float32')\n",
        "            glove_embeddings[word] = vector\n",
        "\n",
        "print(f\"Found {len(glove_embeddings)} words in GloVe\")\n",
        "\n",
        "# Create word-to-index mapping\n",
        "word_to_idx = {word: idx for idx, word in enumerate(sorted(vocab_words))}\n",
        "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
        "\n",
        "# Create embedding matrix\n",
        "embedding_dim = 300\n",
        "vocab_size = len(vocab_words)\n",
        "embedding_matrix = np.random.randn(vocab_size, embedding_dim).astype(np.float32) * 0.01\n",
        "\n",
        "# Fill in GloVe vectors where available\n",
        "for word, idx in word_to_idx.items():\n",
        "    if word in glove_embeddings:\n",
        "        embedding_matrix[idx] = glove_embeddings[word]\n",
        "\n",
        "print(\"\\nStep 5: Saving files...\")\n",
        "\n",
        "# Save the files with the correct names\n",
        "np.save(pjoin(glove_dir, 'our_vab_data.npy'), embedding_matrix)\n",
        "with open(pjoin(glove_dir, 'our_vab_idx.pkl'), 'wb') as f:\n",
        "    pickle.dump(word_to_idx, f)\n",
        "with open(pjoin(glove_dir, 'our_vab_words.pkl'), 'wb') as f:\n",
        "    pickle.dump(idx_to_word, f)\n",
        "\n",
        "# Also copy the raw GloVe file\n",
        "!cp /content/glove.6B.300d.txt {glove_dir}/glove.6B.300d.txt\n",
        "\n",
        "print(\"\\n✓ GloVe files generated successfully!\")\n",
        "print(f\"  - Vocabulary size: {vocab_size}\")\n",
        "print(f\"  - Embedding dim: {embedding_dim}\")\n",
        "print(f\"  - Matrix shape: {embedding_matrix.shape}\")\n",
        "\n",
        "# Verify the files\n",
        "data = np.load(pjoin(glove_dir, 'our_vab_data.npy'))\n",
        "print(f\"\\n✓ Verification: our_vab_data.npy shape = {data.shape}\")\n",
        "print(f\"  File size: {os.path.getsize(pjoin(glove_dir, 'our_vab_data.npy')) / 1024:.2f} KB\")"
      ],
      "metadata": {
        "id": "tl7wF7ycD3dj",
        "outputId": "f531db43-f1dc-4b9f-e8de-8a88477677c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Downloading GloVe embeddings...\n",
            "/content/glove.6B.z 100%[===================>] 822.24M  5.12MB/s    in 2m 43s  \n",
            "\n",
            "Step 2: Extracting...\n",
            "\n",
            "Step 3: Preparing vocabulary for How2Sign dataset...\n",
            "\n",
            "Total vocabulary size: 4\n",
            "\n",
            "Step 4: Loading GloVe vectors...\n",
            "Found 0 words in GloVe\n",
            "\n",
            "Step 5: Saving files...\n",
            "\n",
            "✓ GloVe files generated successfully!\n",
            "  - Vocabulary size: 4\n",
            "  - Embedding dim: 300\n",
            "  - Matrix shape: (4, 300)\n",
            "\n",
            "✓ Verification: our_vab_data.npy shape = (4, 300)\n",
            "  File size: 4.81 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# FIX: Regenerate GloVe with correct format\n",
        "# ============================================\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pickle\n",
        "from os.path import join as pjoin\n",
        "\n",
        "glove_dir = \"/content/drive/MyDrive/GraduationProject/CodeFiles/SaSOKE/deps/t2m/glove/\"\n",
        "\n",
        "print(\"Reading existing GloVe file...\")\n",
        "glove_file = f\"{glove_dir}/glove.6B.300d.txt\"\n",
        "\n",
        "# Load GloVe vectors\n",
        "glove_dict = {}\n",
        "with open(glove_file, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = np.asarray(values[1:], dtype='float32')\n",
        "        glove_dict[word] = vector\n",
        "\n",
        "print(f\"Loaded {len(glove_dict)} GloVe vectors\")\n",
        "\n",
        "# Create vocabulary - use top 5000 most common words\n",
        "vocab_size = 5000\n",
        "words = list(glove_dict.keys())[:vocab_size]\n",
        "\n",
        "# Add special tokens at the beginning\n",
        "special_tokens = ['sos/OTHER', 'eos/OTHER', 'unk/OTHER']\n",
        "words = special_tokens + words\n",
        "\n",
        "print(f\"Total vocabulary: {len(words)} words\")\n",
        "\n",
        "# Create the correct format:\n",
        "# word2idx: maps word -> index (integer)\n",
        "# idx2word: maps index (integer) -> word\n",
        "word2idx = {word: idx for idx, word in enumerate(words)}\n",
        "idx2word = {idx: word for idx, word in enumerate(words)}\n",
        "\n",
        "# Create embedding matrix\n",
        "embedding_dim = 300\n",
        "vocab_len = len(words)\n",
        "embedding_matrix = np.zeros((vocab_len, embedding_dim), dtype=np.float32)\n",
        "\n",
        "# Fill embeddings\n",
        "for idx, word in enumerate(words):\n",
        "    if word in glove_dict:\n",
        "        embedding_matrix[idx] = glove_dict[word]\n",
        "    else:\n",
        "        # Random init for special tokens\n",
        "        embedding_matrix[idx] = np.random.randn(embedding_dim).astype(np.float32) * 0.01\n",
        "\n",
        "print(\"\\nSaving corrected vocabulary files...\")\n",
        "\n",
        "# Save with correct structure\n",
        "np.save(pjoin(glove_dir, 'our_vab_data.npy'), embedding_matrix)\n",
        "\n",
        "with open(pjoin(glove_dir, 'our_vab_idx.pkl'), 'wb') as f:\n",
        "    pickle.dump(word2idx, f)  # word -> idx mapping\n",
        "\n",
        "with open(pjoin(glove_dir, 'our_vab_words.pkl'), 'wb') as f:\n",
        "    pickle.dump(idx2word, f)  # idx -> word mapping\n",
        "\n",
        "print(\"\\n✅ Fixed vocabulary structure!\")\n",
        "print(f\"  - Vocabulary size: {vocab_len}\")\n",
        "print(f\"  - Embedding shape: {embedding_matrix.shape}\")\n",
        "print(f\"  - word2idx type: {type(word2idx)}\")\n",
        "print(f\"  - Sample word2idx: {list(word2idx.items())[:5]}\")\n",
        "print(f\"  - idx2word type: {type(idx2word)}\")\n",
        "print(f\"  - Sample idx2word: {list(idx2word.items())[:5]}\")\n",
        "\n",
        "# Verify\n",
        "data = np.load(pjoin(glove_dir, 'our_vab_data.npy'))\n",
        "with open(pjoin(glove_dir, 'our_vab_idx.pkl'), 'rb') as f:\n",
        "    loaded_w2i = pickle.load(f)\n",
        "with open(pjoin(glove_dir, 'our_vab_words.pkl'), 'rb') as f:\n",
        "    loaded_i2w = pickle.load(f)\n",
        "\n",
        "print(f\"\\n✓ Verification successful!\")\n",
        "print(f\"  - Embedding data shape: {data.shape}\")\n",
        "print(f\"  - word2idx has {len(loaded_w2i)} entries\")\n",
        "print(f\"  - idx2word has {len(loaded_i2w)} entries\")\n",
        "\n",
        "print(\"\\n✅ Ready to train! Run the training command again.\")"
      ],
      "metadata": {
        "id": "Km_xVQUfFiOt",
        "outputId": "f5db48ac-229f-44b3-b715-210fc9749bd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading existing GloVe file...\n",
            "Loaded 400000 GloVe vectors\n",
            "Total vocabulary: 5003 words\n",
            "\n",
            "Saving corrected vocabulary files...\n",
            "\n",
            "✅ Fixed vocabulary structure!\n",
            "  - Vocabulary size: 5003\n",
            "  - Embedding shape: (5003, 300)\n",
            "  - word2idx type: <class 'dict'>\n",
            "  - Sample word2idx: [('sos/OTHER', 0), ('eos/OTHER', 1), ('unk/OTHER', 2), ('the', 3), (',', 4)]\n",
            "  - idx2word type: <class 'dict'>\n",
            "  - Sample idx2word: [(0, 'sos/OTHER'), (1, 'eos/OTHER'), (2, 'unk/OTHER'), (3, 'the'), (4, ',')]\n",
            "\n",
            "✓ Verification successful!\n",
            "  - Embedding data shape: (5003, 300)\n",
            "  - word2idx has 5003 entries\n",
            "  - idx2word has 5003 entries\n",
            "\n",
            "✅ Ready to train! Run the training command again.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLwsGaOF1uCN"
      },
      "source": [
        "## Train SOKE Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "z3FR92mh1uCN",
        "outputId": "5a9561fb-56bd-44d4-da4b-62dc3128b392",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-27 16:19:56,883 SEED_VALUE: 1234\n",
            "DEBUG: false\n",
            "FULL_CONFIG: false\n",
            "PRECISION: null\n",
            "TRAIN:\n",
            "  SPLIT: train\n",
            "  NUM_WORKERS: 2\n",
            "  BATCH_SIZE: 16\n",
            "  END_EPOCH: 150\n",
            "  RESUME: ''\n",
            "  PRETRAINED_VAE: /content/drive/MyDrive/GraduationProject/CodeFiles/SaSOKE/checkpoints/vae/tokenizer.ckpt\n",
            "  PRETRAINED: ''\n",
            "  OPTIM:\n",
            "    target: AdamW\n",
            "    params:\n",
            "      lr: 0.0002\n",
            "      betas:\n",
            "      - 0.9\n",
            "      - 0.99\n",
            "      weight_decay: 0.0\n",
            "  LR_SCHEDULER:\n",
            "    target: CosineAnnealingLR\n",
            "    params:\n",
            "      T_max: ${TRAIN.END_EPOCH}\n",
            "      eta_min: 1.0e-06\n",
            "  STAGE: lm_pretrain\n",
            "EVAL:\n",
            "  SPLIT: val\n",
            "  BATCH_SIZE: 1\n",
            "  NUM_WORKERS: 16\n",
            "TEST:\n",
            "  CHECKPOINTS: null\n",
            "  SPLIT: test\n",
            "  BATCH_SIZE: 1\n",
            "  NUM_WORKERS: 16\n",
            "  SAVE_PREDICTIONS: true\n",
            "  COUNT_TIME: false\n",
            "  REPLICATION_TIMES: 1\n",
            "  REP_I: 0\n",
            "  FOLDER: results\n",
            "model:\n",
            "  target: mGPT.models.mgpt.MotionGPT\n",
            "  params:\n",
            "    condition: text\n",
            "    task: t2m\n",
            "    lm: ${lm.mbart_h2s_csl_phoenix}\n",
            "    motion_vae: ${vq.re96}\n",
            "    stage: ${TRAIN.STAGE}\n",
            "    debug: ${DEBUG}\n",
            "    codebook_size: ${model.params.motion_vae.params.code_num}\n",
            "    metrics_dict: ${METRIC.TYPE}\n",
            "    hand_vae_cfg: ${vq.hand192}\n",
            "    lm_path: /content/drive/MyDrive/GraduationProject/CodeFiles/SaSOKE/deps/mbart-h2s-csl-phoenix\n",
            "    rhand_vae_cfg: ${vq.hand192}\n",
            "  whisper_path: deps/whisper-large-v2\n",
            "LOSS:\n",
            "  LAMBDA_REC: 1.0\n",
            "  LAMBDA_JOINT: 0.0\n",
            "  LAMBDA_LATENT: 1.0e-05\n",
            "  LAMBDA_KL: 1.0e-05\n",
            "  LAMBDA_GEN: 1.0\n",
            "  LAMBDA_CROSS: 1.0\n",
            "  LAMBDA_CYCLE: 1.0\n",
            "  LAMBDA_PRIOR: 0.0\n",
            "  LAMBDA_VELOCITY: 0.0\n",
            "  LAMBDA_COMMIT: 0.02\n",
            "  ABLATION:\n",
            "    RECONS_LOSS: l1_smooth\n",
            "  LAMBDA_CLS: 1.0\n",
            "  LAMBDA_FEATURE: 1.0\n",
            "METRIC:\n",
            "  TASK: t2m\n",
            "  FORCE_IN_METER: true\n",
            "  DIST_SYNC_ON_STEP: true\n",
            "  MM_NUM_SAMPLES: 100\n",
            "  MM_NUM_REPEATS: 30\n",
            "  MM_NUM_TIMES: 10\n",
            "  DIVERSITY_TIMES: 300\n",
            "  TM2T:\n",
            "    t2m_textencoder:\n",
            "      target: mGPT.archs.tm2t_evaluator.TextEncoderBiGRUCo\n",
            "      params:\n",
            "        word_size: 300\n",
            "        pos_size: 15\n",
            "        hidden_size: 512\n",
            "        output_size: 512\n",
            "    t2m_moveencoder:\n",
            "      target: mGPT.archs.tm2t_evaluator.MovementConvEncoder\n",
            "      params:\n",
            "        input_size: ${eval:${DATASET.NFEATS} - 4}\n",
            "        hidden_size: 512\n",
            "        output_size: 512\n",
            "    t2m_motionencoder:\n",
            "      target: mGPT.archs.tm2t_evaluator.MotionEncoderBiGRUCo\n",
            "      params:\n",
            "        input_size: ${evaluator.tm2t.t2m_moveencoder.params.output_size}\n",
            "        hidden_size: 1024\n",
            "        output_size: 512\n",
            "    t2m_path: /content/drive/MyDrive/GraduationProject/CodeFiles/SaSOKE/deps/t2m/\n",
            "  TYPE:\n",
            "  - TM2TMetrics\n",
            "DATASET:\n",
            "  target: mGPT.data.H2S.H2SDataModule\n",
            "  CODE_PATH: TOKENS_h2s_csl_phoenix\n",
            "  TASK_ROOT: deps/mGPT_instructions\n",
            "  TASK_PATH: ''\n",
            "  NFEATS: 263\n",
            "  KIT:\n",
            "    MAX_MOTION_LEN: 196\n",
            "    MIN_MOTION_LEN: 24\n",
            "    MAX_TEXT_LEN: 20\n",
            "    PICK_ONE_TEXT: true\n",
            "    FRAME_RATE: 12.5\n",
            "    UNIT_LEN: 4\n",
            "    ROOT: datasets/kit-ml\n",
            "    SPLIT_ROOT: datasets/kit-ml\n",
            "    MEAN_STD_PATH: deps/t2m/\n",
            "  HUMANML3D:\n",
            "    MAX_MOTION_LEN: 196\n",
            "    MIN_MOTION_LEN: 40\n",
            "    MAX_TEXT_LEN: 20\n",
            "    PICK_ONE_TEXT: true\n",
            "    FRAME_RATE: 20.0\n",
            "    UNIT_LEN: 4\n",
            "    STD_TEXT: false\n",
            "    ROOT: datasets/humanml3d\n",
            "    SPLIT_ROOT: datasets/humanml3d\n",
            "    MEAN_STD_PATH: deps/t2m/\n",
            "  H2S:\n",
            "    CSL_ROOT: /Volumes/The Storage!/SOKE/data/CSL-Daily\n",
            "    DATASET_NAME: how2sign\n",
            "    FRAME_RATE: 20.0\n",
            "    MAX_MOTION_LEN: 400\n",
            "    MAX_TEXT_LEN: 40\n",
            "    MEAN_PATH: /content/drive/MyDrive/GraduationProject/CodeFiles/SaSOKE/smpl-x/mean.pt\n",
            "    MIN_MOTION_LEN: 40\n",
            "    PHOENIX_ROOT: /Volumes/The Storage!/SOKE/data/Phoenix_2014T\n",
            "    PICK_ONE_TEXT: true\n",
            "    ROOT: /content/drive/MyDrive/GraduationProject/CodeFiles/SaSOKE/data/How2Sign\n",
            "    STD_PATH: /content/drive/MyDrive/GraduationProject/CodeFiles/SaSOKE/smpl-x/std.pt\n",
            "    STD_TEXT: false\n",
            "    UNIT_LEN: 4\n",
            "  SMPL_PATH: deps/smpl\n",
            "  TRANSFORM_PATH: deps/transforms/\n",
            "  WORD_VERTILIZER_PATH: /content/drive/MyDrive/GraduationProject/CodeFiles/SaSOKE/deps/t2m/glove/\n",
            "ABLATION:\n",
            "  use_length: false\n",
            "  predict_ratio: 0.2\n",
            "  inbetween_ratio: 0.25\n",
            "  image_size: 256\n",
            "  VAE_TYPE: actor\n",
            "  VAE_ARCH: encoder_decoder\n",
            "  PE_TYPE: actor\n",
            "  DIFF_PE_TYPE: actor\n",
            "  SKIP_CONNECT: false\n",
            "  MLP_DIST: false\n",
            "  IS_DIST: false\n",
            "  PREDICT_EPSILON: true\n",
            "DEMO:\n",
            "  EXAMPLE: null\n",
            "  TASK: t2m\n",
            "LOGGER:\n",
            "  VAL_EVERY_STEPS: 5\n",
            "  LOGGERS:\n",
            "  - wandb\n",
            "  TENSORBOARD:\n",
            "    target: pytorch_lightning.loggers.TensorBoardLogger\n",
            "    params:\n",
            "      save_dir: ${FOLDER_EXP}\n",
            "      name: tensorboard\n",
            "      version: ''\n",
            "  WANDB:\n",
            "    target: pytorch_lightning.loggers.WandbLogger\n",
            "    params:\n",
            "      project: SLG\n",
            "      offline: false\n",
            "      id: null\n",
            "      version: ''\n",
            "      name: ${NAME}\n",
            "      save_dir: ${FOLDER_EXP}\n",
            "  TYPE:\n",
            "  - wandb\n",
            "ACCELERATOR: gpu\n",
            "DEVICE:\n",
            "- 0\n",
            "NAME: SOKE\n",
            "NUM_NODES: 1\n",
            "vq:\n",
            "  re96:\n",
            "    target: mGPT.archs.mgpt_vq.VQVae\n",
            "    params:\n",
            "      quantizer: ema_reset\n",
            "      code_num: 96\n",
            "      code_dim: 512\n",
            "      output_emb_width: 512\n",
            "      down_t: 2\n",
            "      stride_t: 2\n",
            "      width: 512\n",
            "      depth: 3\n",
            "      dilation_growth_rate: 3\n",
            "      norm: None\n",
            "      activation: relu\n",
            "      nfeats: 43\n",
            "      ablation: ${ABLATION}\n",
            "  hand192:\n",
            "    target: mGPT.archs.mgpt_vq.VQVae\n",
            "    params:\n",
            "      quantizer: ema_reset\n",
            "      code_num: 192\n",
            "      code_dim: 512\n",
            "      output_emb_width: 512\n",
            "      down_t: 2\n",
            "      stride_t: 2\n",
            "      width: 512\n",
            "      depth: 3\n",
            "      dilation_growth_rate: 3\n",
            "      norm: None\n",
            "      activation: relu\n",
            "      nfeats: 45\n",
            "      ablation: ${ABLATION}\n",
            "  default:\n",
            "    target: mGPT.archs.mgpt_vq.VQVae\n",
            "    params:\n",
            "      quantizer: ema_reset\n",
            "      code_num: 512\n",
            "      code_dim: 512\n",
            "      output_emb_width: 512\n",
            "      down_t: 2\n",
            "      stride_t: 2\n",
            "      width: 512\n",
            "      depth: 3\n",
            "      dilation_growth_rate: 3\n",
            "      norm: None\n",
            "      activation: relu\n",
            "      nfeats: ${DATASET.NFEATS}\n",
            "      ablation: ${ABLATION}\n",
            "evaluator:\n",
            "  tm2t:\n",
            "    t2m_textencoder:\n",
            "      target: mGPT.archs.tm2t_evaluator.TextEncoderBiGRUCo\n",
            "      params:\n",
            "        word_size: 300\n",
            "        pos_size: 15\n",
            "        hidden_size: 512\n",
            "        output_size: 512\n",
            "    t2m_moveencoder:\n",
            "      target: mGPT.archs.tm2t_evaluator.MovementConvEncoder\n",
            "      params:\n",
            "        input_size: ${eval:${DATASET.NFEATS} - 4}\n",
            "        hidden_size: 512\n",
            "        output_size: 512\n",
            "    t2m_motionencoder:\n",
            "      target: mGPT.archs.tm2t_evaluator.MotionEncoderBiGRUCo\n",
            "      params:\n",
            "        input_size: ${evaluator.tm2t.t2m_moveencoder.params.output_size}\n",
            "        hidden_size: 1024\n",
            "        output_size: 512\n",
            "lm:\n",
            "  mbart_h2s_csl_phoenix:\n",
            "    target: mGPT.archs.mgpt_mbart.Mbart_Based_MLM\n",
            "    params:\n",
            "      model_type: mbart_multi\n",
            "      model_path: ./deps/mbart-h2s-csl-phoenix\n",
            "      num_heads: 3\n",
            "      stage: ${TRAIN.STAGE}\n",
            "      motion_codebook_size: ${model.params.codebook_size}\n",
            "      ablation: ${ABLATION}\n",
            "  default:\n",
            "    target: mGPT.archs.mgpt_lm.MLM\n",
            "    params:\n",
            "      model_type: t5\n",
            "      model_path: ./deps/flan-t5-base\n",
            "      stage: ${TRAIN.STAGE}\n",
            "      motion_codebook_size: ${model.params.codebook_size}\n",
            "      ablation: ${ABLATION}\n",
            "CONFIG_FOLDER: configs\n",
            "FOLDER: experiments\n",
            "RENDER:\n",
            "  BLENDER_PATH: libs/blender-2.93.2-linux-x64/blender\n",
            "  SMPL_MODEL_PATH: deps/smpl/smpl_models/smpl\n",
            "  MODEL_PATH: deps/smpl/smpl_models/\n",
            "  FACES_PATH: deps/smplh/smplh.faces\n",
            "USE_GPUS: 2,3,4,5,6,7\n",
            "FOLDER_EXP: experiments/mgpt/SOKE\n",
            "TIME: 2025-11-27-16-19-56\n",
            "\n",
            "Seed set to 1234\n",
            "2025-11-27 16:19:56,889 Callbacks initialized\n",
            "mean path /content/drive/MyDrive/GraduationProject/CodeFiles/SaSOKE/smpl-x/mean.pt std_path:  /content/drive/MyDrive/GraduationProject/CodeFiles/SaSOKE/smpl-x/std.pt\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/content/SaSOKE/train.py\", line 99, in <module>\n",
            "    main()\n",
            "  File \"/content/SaSOKE/train.py\", line 43, in main\n",
            "    datamodule = build_data(cfg)\n",
            "                 ^^^^^^^^^^^^^^^\n",
            "  File \"/content/SaSOKE/mGPT/data/build_data.py\", line 10, in build_data\n",
            "    return instantiate_from_config(data_config)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/SaSOKE/mGPT/config.py\", line 42, in instantiate_from_config\n",
            "    return get_obj_from_str(config[\"target\"])(**config.get(\"params\", dict()))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/SaSOKE/mGPT/data/H2S.py\", line 74, in __init__\n",
            "    self.hparams.w_vectorizer = WordVectorizer(\n",
            "                                ^^^^^^^^^^^^^^^\n",
            "  File \"/content/SaSOKE/mGPT/data/humanml/utils/word_vectorizer.py\", line 50, in __init__\n",
            "    self.word2vec = {w: vectors[word2idx[w]] for w in words}\n",
            "                                ~~~~~~~~^^^\n",
            "KeyError: 0\n"
          ]
        }
      ],
      "source": [
        "# Start training\n",
        "!python -m train --cfg configs/soke_colab.yaml --nodebug"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNyFUOVB1uCO"
      },
      "source": [
        "## Monitor Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kan7chL71uCO"
      },
      "outputs": [],
      "source": [
        "# Load tensorboard\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir experiments/mgpt/SOKE/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HXufZL_1uCO"
      },
      "source": [
        "## Test Model\n",
        "Run inference after training completes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mn9EoGzB1uCO"
      },
      "outputs": [],
      "source": [
        "# Run inference on test set\n",
        "!python -m test --cfg configs/soke_colab.yaml --task t2m\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}