{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/SattamAltwaim/SaSOKE/blob/main/notebooks/8_gradio_web_ui.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83e\udd1f SOKE Gradio Web UI\n\n**Simple Web UI that ACTUALLY WORKS in Colab!**\n\nGradio is designed specifically for Colab - no proxy issues!\n\n### Features\n- \u2705 Works 100% in Colab (no blank pages!)\n- \u2705 Beautiful UI\n- \u2705 Public shareable link\n- \u2705 3D visualization\n\n### Requirements\n- **GPU Runtime**: `Runtime \u2192 Change runtime type \u2192 GPU`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone repo and mount Drive\nimport os\nif not os.path.exists('/content/SaSOKE'):\n    !git clone https://github.com/SattamAltwaim/SaSOKE.git\n%cd /content/SaSOKE\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\ndrive_data = '/content/drive/MyDrive/GraduationProject/CodeFiles/SaSOKE'\nprint(\"\u2713 Ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n!pip install -q pytorch_lightning torchmetrics omegaconf shortuuid transformers diffusers einops wandb rich matplotlib\n!pip install -q smplx h5py scikit-image spacy ftfy more-itertools natsort tensorboard sentencepiece\n!pip install -q gradio\nprint(\"\u2713 Dependencies installed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup paths\nimport sys\nimport yaml\nfrom mGPT.config import parse_args\n\ndeps_links = {\n    'deps/smpl_models': f'{drive_data}/deps/smpl_models',\n    'deps/mbart-h2s-csl-phoenix': f'{drive_data}/deps/mbart-h2s-csl-phoenix',\n}\n\nfor expected_path, actual_path in deps_links.items():\n    if not os.path.exists(expected_path):\n        os.makedirs(os.path.dirname(expected_path), exist_ok=True)\n        os.symlink(actual_path, expected_path)\n\nwith open('configs/soke.yaml', 'r') as f:\n    config = yaml.safe_load(f)\n\nconfig['ACCELERATOR'] = 'gpu'\nconfig['DEVICE'] = [0]\nconfig['DATASET']['H2S']['ROOT'] = f'{drive_data}/data/How2Sign'\nconfig['DATASET']['H2S']['MEAN_PATH'] = f'{drive_data}/smpl-x/mean.pt'\nconfig['DATASET']['H2S']['STD_PATH'] = f'{drive_data}/smpl-x/std.pt'\nconfig['TRAIN']['PRETRAINED_VAE'] = f'{drive_data}/checkpoints/vae/tokenizer.ckpt'\n\nwith open('configs/web_inference.yaml', 'w') as f:\n    yaml.dump(config, f)\n\nwith open('configs/assets.yaml', 'r') as f:\n    assets = yaml.safe_load(f)\n\nassets['RENDER']['SMPL_MODEL_PATH'] = 'deps/smpl_models/smpl'\nassets['RENDER']['MODEL_PATH'] = 'deps/smpl_models'\nassets['METRIC']['TM2T']['t2m_path'] = f'{drive_data}/deps/deps/t2m/t2m/'\n\nwith open('configs/assets_web.yaml', 'w') as f:\n    yaml.dump(assets, f)\n\nsys.argv = ['', '--cfg', 'configs/web_inference.yaml', '--cfg_assets', 'configs/assets_web.yaml']\ncfg = parse_args(phase=\"test\")\ncfg.FOLDER = cfg.TEST.FOLDER\nprint(\"\u2713 Configuration ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model\nimport torch\nimport pytorch_lightning as pl\nfrom mGPT.models.build_model import build_model\nfrom mGPT.data.build_data import build_data\nfrom mGPT.utils.load_checkpoint import load_pretrained_vae, load_pretrained\nfrom mGPT.utils.logger import create_logger\nfrom mGPT.utils.human_models import smpl_x, get_coord\n\npl.seed_everything(cfg.SEED_VALUE)\ncfg.DATASET.WORD_VERTILIZER_PATH = f'{drive_data}/deps/deps/t2m/glove/'\n\ndatamodule = build_data(cfg)\nmodel = build_model(cfg, datamodule)\n\nlogger = create_logger(cfg, phase=\"test\")\nif cfg.TRAIN.PRETRAINED_VAE:\n    load_pretrained_vae(cfg, model, logger)\n\nckpt_path = f'{drive_data}/experiments/mgpt/SOKE/checkpoints/last.ckpt'\nif os.path.exists(ckpt_path):\n    cfg.TEST.CHECKPOINTS = ckpt_path\n    load_pretrained(cfg, model, logger, phase=\"test\")\n\nmodel = model.cuda()\nmodel.eval()\n\nmean = datamodule.hparams.mean.cuda()\nstd = datamodule.hparams.std.cuda()\n\nprint(\"\\n\u2705 Model loaded and ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Create Gradio Interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gradio as gr\nimport numpy as np\nimport time\nimport json\n\ndef feats_to_smplx(features, mean_tensor, std_tensor):\n    features = features * std_tensor + mean_tensor\n    T = features.shape[0]\n    zero_pose = torch.zeros(T, 36).to(features)\n    return torch.cat([zero_pose, features], dim=-1)\n\ndef generate_mesh(smplx_params_full):\n    num_frames = smplx_params_full.shape[0]\n    all_vertices = []\n    shape_param = torch.tensor([[-0.07284723, 0.1795129, -0.27608207, 0.135155, 0.10748172,\n                                 0.16037364, -0.01616933, -0.03450319, 0.01369138, 0.01108842]]).float()\n    \n    for i in range(num_frames):\n        frame_params = smplx_params_full[i:i+1]\n        with torch.no_grad():\n            vertices, _ = get_coord(\n                root_pose=frame_params[:, 0:3],\n                body_pose=frame_params[:, 3:66],\n                lhand_pose=frame_params[:, 66:111],\n                rhand_pose=frame_params[:, 111:156],\n                jaw_pose=frame_params[:, 156:159],\n                shape=shape_param,\n                expr=frame_params[:, 159:169]\n            )\n        all_vertices.append(vertices[0].cpu().numpy())\n    \n    return np.array(all_vertices), smpl_x.face\n\ndef text_to_sign(text, sign_language):\n    if not text.strip():\n        return None, \"\u26a0\ufe0f Please enter some text\", \"\"\n    \n    start_time = time.time()\n    \n    try:\n        batch = {'text': [text], 'length': [0], 'src': [sign_language]}\n        \n        with torch.no_grad():\n            output = model.forward(batch, task=\"t2m\")\n        \n        feats = output['feats'][0] if 'feats' in output else None\n        \n        if feats is None:\n            return None, \"\u274c Generation failed - no features produced\", \"\"\n        \n        smplx_params = feats_to_smplx(feats, mean, std)\n        vertices, faces = generate_mesh(smplx_params)\n        \n        num_frames = vertices.shape[0]\n        gen_time = time.time() - start_time\n        \n        # Create HTML viewer\n        html = create_3d_viewer(vertices, faces)\n        \n        status = f\"\u2705 Generated {num_frames} frames in {gen_time:.2f}s\"\n        info = f\"\ud83d\udcca Frames: {num_frames} | Duration: {num_frames/20:.2f}s | FPS: 20\"\n        \n        return html, status, info\n        \n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return None, f\"\u274c Error: {str(e)}\", \"\"\n\ndef create_3d_viewer(vertices, faces):\n    \"\"\"Create an interactive 3D viewer HTML\"\"\"\n    \n    # Convert to JSON\n    vertices_json = json.dumps(vertices.tolist())\n    faces_json = json.dumps(faces.tolist())\n    \n    html = f\"\"\"\n    <!DOCTYPE html>\n    <html>\n    <head>\n        <script src=\"https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js\"></script>\n        <script src=\"https://cdn.jsdelivr.net/npm/three@0.128.0/examples/js/controls/OrbitControls.js\"></script>\n        <style>\n            body {{ margin: 0; overflow: hidden; background: #1a1a24; }}\n            #canvas {{ width: 100%; height: 600px; }}\n            .controls {{\n                position: absolute;\n                bottom: 20px;\n                left: 50%;\n                transform: translateX(-50%);\n                background: rgba(0,0,0,0.7);\n                padding: 15px;\n                border-radius: 10px;\n                display: flex;\n                gap: 10px;\n                align-items: center;\n            }}\n            button {{\n                padding: 10px 20px;\n                border: none;\n                border-radius: 5px;\n                background: #6366f1;\n                color: white;\n                cursor: pointer;\n                font-size: 16px;\n            }}\n            button:hover {{ background: #4f46e5; }}\n            button:disabled {{ opacity: 0.5; cursor: not-allowed; }}\n            .info {{ color: white; font-family: monospace; font-size: 14px; }}\n        </style>\n    </head>\n    <body>\n        <div id=\"canvas\"></div>\n        <div class=\"controls\">\n            <button id=\"play\">\u25b6 Play</button>\n            <button id=\"pause\" disabled>\u23f8 Pause</button>\n            <button id=\"reset\">\u21ba Reset</button>\n            <span class=\"info\" id=\"frame-info\">Frame: 1 / {vertices.shape[0]}</span>\n            <select id=\"speed\">\n                <option value=\"0.5\">0.5x</option>\n                <option value=\"1\" selected>1x</option>\n                <option value=\"2\">2x</option>\n            </select>\n        </div>\n        <script>\n            const vertices = {vertices_json};\n            const faces = {faces_json};\n            const FPS = 20;\n            let currentFrame = 0;\n            let isPlaying = false;\n            let playbackSpeed = 1;\n            let animationId = null;\n            \n            // Setup Three.js\n            const scene = new THREE.Scene();\n            scene.background = new THREE.Color(0x1a1a24);\n            \n            const container = document.getElementById('canvas');\n            const camera = new THREE.PerspectiveCamera(45, container.clientWidth / 600, 0.1, 100);\n            camera.position.set(0, 0.5, 2.5);\n            \n            const renderer = new THREE.WebGLRenderer({{ antialias: true }});\n            renderer.setSize(container.clientWidth, 600);\n            container.appendChild(renderer.domElement);\n            \n            const controls = new THREE.OrbitControls(camera, renderer.domElement);\n            controls.target.set(0, 0.5, 0);\n            \n            scene.add(new THREE.AmbientLight(0xffffff, 0.5));\n            const light = new THREE.DirectionalLight(0xffffff, 0.8);\n            light.position.set(2, 3, 2);\n            scene.add(light);\n            \n            const grid = new THREE.GridHelper(4, 20, 0x333344, 0x222233);\n            grid.position.y = -0.5;\n            scene.add(grid);\n            \n            // Create mesh\n            const geometry = new THREE.BufferGeometry();\n            geometry.setAttribute('position', new THREE.BufferAttribute(new Float32Array(vertices[0].flat()), 3));\n            geometry.setIndex(new THREE.BufferAttribute(new Uint32Array(faces.flat()), 1));\n            geometry.computeVertexNormals();\n            \n            const material = new THREE.MeshPhongMaterial({{\n                color: 0x6366f1,\n                shininess: 30,\n                side: THREE.DoubleSide\n            }});\n            \n            const mesh = new THREE.Mesh(geometry, material);\n            scene.add(mesh);\n            \n            function updateMesh(frame) {{\n                geometry.setAttribute('position', new THREE.BufferAttribute(new Float32Array(vertices[frame].flat()), 3));\n                geometry.computeVertexNormals();\n                document.getElementById('frame-info').textContent = `Frame: ${{frame + 1}} / ${{vertices.length}}`;\n            }}\n            \n            function animate() {{\n                requestAnimationFrame(animate);\n                controls.update();\n                renderer.render(scene, camera);\n            }}\n            animate();\n            \n            function play() {{\n                isPlaying = true;\n                document.getElementById('play').disabled = true;\n                document.getElementById('pause').disabled = false;\n                \n                let lastTime = performance.now();\n                function step(time) {{\n                    if (!isPlaying) return;\n                    if (time - lastTime >= 1000 / (FPS * playbackSpeed)) {{\n                        currentFrame = (currentFrame + 1) % vertices.length;\n                        updateMesh(currentFrame);\n                        lastTime = time;\n                    }}\n                    animationId = requestAnimationFrame(step);\n                }}\n                animationId = requestAnimationFrame(step);\n            }}\n            \n            function pause() {{\n                isPlaying = false;\n                document.getElementById('play').disabled = false;\n                document.getElementById('pause').disabled = true;\n                if (animationId) cancelAnimationFrame(animationId);\n            }}\n            \n            function reset() {{\n                pause();\n                currentFrame = 0;\n                updateMesh(0);\n            }}\n            \n            document.getElementById('play').onclick = play;\n            document.getElementById('pause').onclick = pause;\n            document.getElementById('reset').onclick = reset;\n            document.getElementById('speed').onchange = (e) => {{ playbackSpeed = parseFloat(e.target.value); }};\n            \n            window.addEventListener('resize', () => {{\n                camera.aspect = container.clientWidth / 600;\n                camera.updateProjectionMatrix();\n                renderer.setSize(container.clientWidth, 600);\n            }});\n        </script>\n    </body>\n    </html>\n    \"\"\"\n    return html\n\nprint(\"\u2713 Functions ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Launch Interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Gradio interface\nwith gr.Blocks(theme=gr.themes.Soft(), title=\"SOKE - Text to Sign Language\") as demo:\n    gr.Markdown(\"\"\"\n    # \ud83e\udd1f SOKE - Text to Sign Language\n    \n    **Generate sign language animations from text in real-time!**\n    \n    Enter your text below and click Generate to see the 3D animation.\n    \"\"\")\n    \n    with gr.Row():\n        with gr.Column(scale=1):\n            text_input = gr.Textbox(\n                label=\"Text to Translate\",\n                placeholder=\"Type your message here...\",\n                lines=3,\n                value=\"Hello, how are you today?\"\n            )\n            \n            language_select = gr.Dropdown(\n                label=\"Target Sign Language\",\n                choices=[\n                    (\"\ud83c\uddfa\ud83c\uddf8 American Sign Language (ASL)\", \"how2sign\"),\n                    (\"\ud83c\udde8\ud83c\uddf3 Chinese Sign Language (CSL)\", \"csl\"),\n                    (\"\ud83c\udde9\ud83c\uddea German Sign Language (DGS)\", \"phoenix\")\n                ],\n                value=\"how2sign\"\n            )\n            \n            generate_btn = gr.Button(\"\ud83d\ude80 Generate Sign Language\", variant=\"primary\", size=\"lg\")\n            \n            status_text = gr.Markdown(\"\")\n            info_text = gr.Markdown(\"\")\n            \n            gr.Markdown(\"\"\"\n            ### \ud83d\udca1 Try these examples:\n            - Hello, how are you?\n            - Thank you for your help\n            - Nice to meet you!\n            - What is your name?\n            \"\"\")\n        \n        with gr.Column(scale=2):\n            viewer = gr.HTML(label=\"3D Visualization\")\n    \n    generate_btn.click(\n        fn=text_to_sign,\n        inputs=[text_input, language_select],\n        outputs=[viewer, status_text, info_text]\n    )\n\n# Launch with share=True for public URL\nprint(\"\\n\" + \"=\"*70)\nprint(\"\ud83d\ude80 LAUNCHING GRADIO INTERFACE...\")\nprint(\"=\"*70)\nprint(\"\\n\u23f3 Please wait while Gradio starts...\")\nprint(\"\\n\ud83d\udca1 A public URL will be generated that you can share with anyone!\")\nprint(\"\\n\" + \"=\"*70)\n\ndemo.launch(share=True, debug=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}