{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/SattamAltwaim/SaSOKE/blob/main/notebooks/7_simple_web_ui.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83e\udd1f SOKE Simple Web UI\n\n**Super Simple Setup - Just Upload HTML and Go!**\n\nThis notebook:\n1. Loads the SOKE model\n2. Starts an API server\n3. Hosts your HTML file\n4. Gives you a shareable URL\n\n### Requirements\n- **GPU Runtime**: `Runtime \u2192 Change runtime type \u2192 GPU`\n- **HTML File**: Upload `soke_ui.html` when prompted"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone repo and mount Drive\nimport os\nif not os.path.exists('/content/SaSOKE'):\n    !git clone https://github.com/SattamAltwaim/SaSOKE.git\n%cd /content/SaSOKE\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\ndrive_data = '/content/drive/MyDrive/GraduationProject/CodeFiles/SaSOKE'\nprint(\"\u2713 Ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n!pip install -q pytorch_lightning torchmetrics omegaconf shortuuid transformers diffusers einops wandb rich matplotlib\n!pip install -q smplx h5py scikit-image spacy ftfy more-itertools natsort tensorboard sentencepiece\n!pip install -q fastapi uvicorn python-multipart nest_asyncio\nprint(\"\u2713 Dependencies installed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup paths\nimport sys\nimport yaml\nfrom omegaconf import OmegaConf\nfrom mGPT.config import parse_args\n\n# Create symbolic links\ndeps_links = {\n    'deps/smpl_models': f'{drive_data}/deps/smpl_models',\n    'deps/mbart-h2s-csl-phoenix': f'{drive_data}/deps/mbart-h2s-csl-phoenix',\n}\n\nfor expected_path, actual_path in deps_links.items():\n    if not os.path.exists(expected_path):\n        os.makedirs(os.path.dirname(expected_path), exist_ok=True)\n        os.symlink(actual_path, expected_path)\n\n# Configure\nwith open('configs/soke.yaml', 'r') as f:\n    config = yaml.safe_load(f)\n\nconfig['ACCELERATOR'] = 'gpu'\nconfig['DEVICE'] = [0]\nconfig['DATASET']['H2S']['ROOT'] = f'{drive_data}/data/How2Sign'\nconfig['DATASET']['H2S']['MEAN_PATH'] = f'{drive_data}/smpl-x/mean.pt'\nconfig['DATASET']['H2S']['STD_PATH'] = f'{drive_data}/smpl-x/std.pt'\nconfig['TRAIN']['PRETRAINED_VAE'] = f'{drive_data}/checkpoints/vae/tokenizer.ckpt'\n\nwith open('configs/web_inference.yaml', 'w') as f:\n    yaml.dump(config, f)\n\nwith open('configs/assets.yaml', 'r') as f:\n    assets = yaml.safe_load(f)\n\nassets['RENDER']['SMPL_MODEL_PATH'] = 'deps/smpl_models/smpl'\nassets['RENDER']['MODEL_PATH'] = 'deps/smpl_models'\nassets['METRIC']['TM2T']['t2m_path'] = f'{drive_data}/deps/deps/t2m/t2m/'\n\nwith open('configs/assets_web.yaml', 'w') as f:\n    yaml.dump(assets, f)\n\nsys.argv = ['', '--cfg', 'configs/web_inference.yaml', '--cfg_assets', 'configs/assets_web.yaml']\ncfg = parse_args(phase=\"test\")\ncfg.FOLDER = cfg.TEST.FOLDER\n\nprint(\"\u2713 Configuration ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model\nimport torch\nimport pytorch_lightning as pl\nfrom mGPT.models.build_model import build_model\nfrom mGPT.data.build_data import build_data\nfrom mGPT.utils.load_checkpoint import load_pretrained_vae, load_pretrained\nfrom mGPT.utils.logger import create_logger\n\npl.seed_everything(cfg.SEED_VALUE)\ncfg.DATASET.WORD_VERTILIZER_PATH = f'{drive_data}/deps/deps/t2m/glove/'\n\ndatamodule = build_data(cfg)\nmodel = build_model(cfg, datamodule)\n\nlogger = create_logger(cfg, phase=\"test\")\nif cfg.TRAIN.PRETRAINED_VAE:\n    load_pretrained_vae(cfg, model, logger)\n\nckpt_path = f'{drive_data}/experiments/mgpt/SOKE/checkpoints/last.ckpt'\nif os.path.exists(ckpt_path):\n    cfg.TEST.CHECKPOINTS = ckpt_path\n    load_pretrained(cfg, model, logger, phase=\"test\")\n\nmodel = model.cuda()\nmodel.eval()\n\nmean = datamodule.hparams.mean.cuda()\nstd = datamodule.hparams.std.cuda()\n\n# Load SMPL-X\nfrom mGPT.utils.human_models import smpl_x, get_coord\n\nprint(\"\\n\u2705 Model loaded and ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Upload Your HTML File\n\n**Upload the `soke_ui.html` file (or any HTML file you want to host)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\nimport shutil\n\nprint(\"\ud83d\udce4 Please upload your HTML file...\")\nuploaded = files.upload()\n\nif uploaded:\n    html_filename = list(uploaded.keys())[0]\n    \n    # Save to static folder\n    os.makedirs('static', exist_ok=True)\n    shutil.copy(html_filename, 'static/index.html')\n    \n    print(f\"\\n\u2705 HTML file uploaded: {html_filename}\")\n    print(f\"   Saved as: static/index.html\")\nelse:\n    print(\"\\n\u26a0\ufe0f No file uploaded. The server will start but won't have a UI.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Start Server\n\n**This will start the server and give you a URL to share!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create FastAPI app with static file serving\nfrom fastapi import FastAPI, HTTPException\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import FileResponse, HTMLResponse\nfrom fastapi.staticfiles import StaticFiles\nfrom pydantic import BaseModel\nimport time\n\napp = FastAPI(title=\"SOKE API\")\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# Serve static files\nif os.path.exists('static'):\n    app.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\n\nclass TextRequest(BaseModel):\n    text: str\n    sign_language: str = \"how2sign\"\n    fps: int = 20\n    include_mesh: bool = True\n\n@app.get(\"/\")\nasync def root():\n    if os.path.exists('static/index.html'):\n        return FileResponse('static/index.html')\n    return HTMLResponse(\"<h1>SOKE Server Running</h1><p>Upload an HTML file to see the UI.</p>\")\n\n@app.get(\"/health\")\nasync def health():\n    return {\"status\": \"healthy\", \"model_loaded\": True, \"gpu_available\": torch.cuda.is_available()}\n\n@app.get(\"/languages\")\nasync def languages():\n    return {\"languages\": [\n        {\"id\": \"how2sign\", \"name\": \"American Sign Language (ASL)\", \"input_language\": \"English\"},\n        {\"id\": \"csl\", \"name\": \"Chinese Sign Language (CSL)\", \"input_language\": \"Chinese\"},\n        {\"id\": \"phoenix\", \"name\": \"German Sign Language (DGS)\", \"input_language\": \"German\"}\n    ]}\n\ndef feats_to_smplx_api(features, mean_tensor, std_tensor):\n    features = features * std_tensor + mean_tensor\n    T = features.shape[0]\n    zero_pose = torch.zeros(T, 36).to(features)\n    features_full = torch.cat([zero_pose, features], dim=-1)\n    return {\n        'root_pose': features_full[:, 0:3].cpu().numpy().tolist(),\n        'body_pose': features_full[:, 3:66].cpu().numpy().tolist(),\n        'lhand_pose': features_full[:, 66:111].cpu().numpy().tolist(),\n        'rhand_pose': features_full[:, 111:156].cpu().numpy().tolist(),\n        'jaw_pose': features_full[:, 156:159].cpu().numpy().tolist(),\n        'expression': features_full[:, 159:169].cpu().numpy().tolist(),\n    }\n\ndef generate_mesh_api(smplx_params):\n    num_frames = len(smplx_params['body_pose'])\n    all_vertices = []\n    shape_param = torch.tensor([[-0.07284723, 0.1795129, -0.27608207, 0.135155, 0.10748172,\n                                 0.16037364, -0.01616933, -0.03450319, 0.01369138, 0.01108842]]).float()\n    for i in range(num_frames):\n        root_pose = torch.tensor([smplx_params['root_pose'][i]]).float()\n        body_pose = torch.tensor([smplx_params['body_pose'][i]]).float()\n        lhand_pose = torch.tensor([smplx_params['lhand_pose'][i]]).float()\n        rhand_pose = torch.tensor([smplx_params['rhand_pose'][i]]).float()\n        jaw_pose = torch.tensor([smplx_params['jaw_pose'][i]]).float()\n        expression = torch.tensor([smplx_params['expression'][i]]).float()\n        with torch.no_grad():\n            vertices, _ = get_coord(root_pose=root_pose, body_pose=body_pose, lhand_pose=lhand_pose,\n                                   rhand_pose=rhand_pose, jaw_pose=jaw_pose, shape=shape_param, expr=expression)\n        all_vertices.append(vertices[0].cpu().numpy().tolist())\n    return {\"vertices\": all_vertices, \"faces\": smpl_x.face.tolist()}\n\n@app.post(\"/generate\")\nasync def generate(request: TextRequest):\n    start_time = time.time()\n    try:\n        batch = {'text': [request.text], 'length': [0], 'src': [request.sign_language]}\n        with torch.no_grad():\n            output = model.forward(batch, task=\"t2m\")\n        feats = output['feats'][0] if 'feats' in output else None\n        if feats is None:\n            return {\"success\": False, \"error\": \"No features generated\", \"text\": request.text, \n                    \"num_frames\": 0, \"fps\": request.fps, \"generation_time\": time.time() - start_time}\n        smplx_params = feats_to_smplx_api(feats, mean, std)\n        num_frames = len(smplx_params['body_pose'])\n        mesh_data = generate_mesh_api(smplx_params) if request.include_mesh else None\n        return {\n            \"success\": True, \"text\": request.text, \"num_frames\": num_frames, \"fps\": request.fps,\n            \"smplx_params\": smplx_params, \"mesh_data\": mesh_data,\n            \"generation_time\": time.time() - start_time\n        }\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return {\"success\": False, \"error\": str(e), \"text\": request.text, \n                \"num_frames\": 0, \"fps\": request.fps, \"generation_time\": time.time() - start_time}\n\nprint(\"\u2713 API configured!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start the server\nimport nest_asyncio\nimport uvicorn\nfrom threading import Thread\nimport socket\n\nnest_asyncio.apply()\n\ndef find_free_port(start_port=8080):\n    for port in range(start_port, start_port + 100):\n        try:\n            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n                s.bind(('0.0.0.0', port))\n                return port\n        except OSError:\n            continue\n    return None\n\nPORT = find_free_port(8080)\n\ndef run_server():\n    uvicorn.run(app, host=\"0.0.0.0\", port=PORT, log_level=\"warning\")\n\nserver_thread = Thread(target=run_server, daemon=True)\nserver_thread.start()\n\nimport time\ntime.sleep(3)\n\n# Get Colab URL\nfrom google.colab.output import eval_js\ncolab_url = eval_js(f'google.colab.kernel.proxyPort({PORT})')\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"\u2705 SERVER IS RUNNING!\")\nprint(\"=\" * 70)\nprint(f\"\\n\ud83c\udf10 YOUR URL (share this with anyone):\")\nprint(f\"\\n   {colab_url}\")\nprint(f\"\\n\" + \"=\" * 70)\nprint(\"\\n\ud83d\udccc IMPORTANT:\")\nprint(\"   - This URL is public and shareable\")\nprint(\"   - It will stay active as long as this cell is running\")\nprint(\"   - Don't stop this cell or the server will stop\")\nprint(\"\\n\ud83d\udca1 TIP: Open the URL in a new tab to test it!\")\nprint(\"\\n\u26a0\ufe0f  Keep this cell running. Run the KEEP ALIVE cell below to prevent timeout.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Keep Server Running\n\n**Run this cell to keep the server alive (prevents Colab timeout)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Keep-alive loop\nimport time\nimport requests\n\nprint(\"\ud83d\udd04 Keep-alive active. Press \u23f9 to stop.\\n\")\n\ntry:\n    while True:\n        try:\n            response = requests.get(f\"http://localhost:{PORT}/health\", timeout=10)\n            status = response.json()\n            print(f\"\\r[{time.strftime('%H:%M:%S')}] \u2713 Running | GPU: {status['gpu_available']}\", end=\"\", flush=True)\n        except:\n            print(f\"\\r[{time.strftime('%H:%M:%S')}] \u26a0 Checking...\", end=\"\", flush=True)\n        time.sleep(60)\nexcept KeyboardInterrupt:\n    print(\"\\n\\n\u2713 Stopped.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}