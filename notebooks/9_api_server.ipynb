{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmioyDv6faa7"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SattamAltwaim/SaSOKE/blob/main/notebooks/9_api_server.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBH2eudafaa_"
      },
      "source": [
        "# ü§ü SOKE API Server\n",
        "\n",
        "**Flask API that provides SMPL-X parameters and GLB frames for sign language generation**\n",
        "\n",
        "### Features\n",
        "- ‚úÖ REST API endpoint for text-to-sign generation\n",
        "- ‚úÖ Returns GLB frames (base64 encoded) ready for 3D display\n",
        "- ‚úÖ CORS enabled for frontend access\n",
        "- ‚úÖ Works with standalone Apple-style frontend\n",
        "\n",
        "### Requirements\n",
        "- **GPU Runtime**: `Runtime ‚Üí Change runtime type ‚Üí GPU`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p489veNnfabA"
      },
      "source": [
        "## Step 1: Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7gjt4BbcfabA",
        "outputId": "275b87a9-54a8-49e0-b64f-ce5aac1daddf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SaSOKE'...\n",
            "remote: Enumerating objects: 580, done.\u001b[K\n",
            "remote: Counting objects: 100% (85/85), done.\u001b[K\n",
            "remote: Compressing objects: 100% (65/65), done.\u001b[K\n",
            "remote: Total 580 (delta 50), reused 43 (delta 20), pack-reused 495 (from 1)\u001b[K\n",
            "Receiving objects: 100% (580/580), 2.56 MiB | 11.38 MiB/s, done.\n",
            "Resolving deltas: 100% (265/265), done.\n",
            "/content/SaSOKE\n",
            "Mounted at /content/drive\n",
            "‚úì Ready!\n"
          ]
        }
      ],
      "source": [
        "# Clone repo and mount Drive\n",
        "import os\n",
        "if not os.path.exists('/content/SaSOKE'):\n",
        "    !git clone https://github.com/SattamAltwaim/SaSOKE.git\n",
        "%cd /content/SaSOKE\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "drive_data = '/content/drive/MyDrive/GraduationProject/CodeFiles/SaSOKE'\n",
        "print(\"‚úì Ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "g7sdtXDOfabC",
        "outputId": "3db3933b-76d9-4935-85d4-fa4d95f9cae6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/849.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m839.7/849.5 kB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m849.5/849.5 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m736.6/736.6 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h‚úì Dependencies installed!\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "!pip install -q pytorch_lightning torchmetrics omegaconf shortuuid transformers diffusers einops wandb rich matplotlib\n",
        "!pip install -q smplx h5py scikit-image spacy ftfy more-itertools natsort tensorboard sentencepiece\n",
        "!pip install -q flask flask-cors trimesh\n",
        "print(\"‚úì Dependencies installed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OZJmp0SfabD"
      },
      "source": [
        "## Step 2: Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4_WUojN1fabD",
        "outputId": "46ccd0a3-2e6b-4894-94d6-77385d1c14cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Force no debugging when testing\n",
            "‚úì Configuration ready!\n"
          ]
        }
      ],
      "source": [
        "# Setup paths\n",
        "import sys\n",
        "import yaml\n",
        "from mGPT.config import parse_args\n",
        "\n",
        "deps_links = {\n",
        "    'deps/smpl_models': f'{drive_data}/deps/smpl_models',\n",
        "    'deps/mbart-h2s-csl-phoenix': f'{drive_data}/deps/mbart-h2s-csl-phoenix',\n",
        "}\n",
        "\n",
        "for expected_path, actual_path in deps_links.items():\n",
        "    if not os.path.exists(expected_path):\n",
        "        os.makedirs(os.path.dirname(expected_path), exist_ok=True)\n",
        "        os.symlink(actual_path, expected_path)\n",
        "\n",
        "with open('configs/soke.yaml', 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "config['ACCELERATOR'] = 'gpu'\n",
        "config['DEVICE'] = [0]\n",
        "config['DATASET']['H2S']['ROOT'] = f'{drive_data}/data/How2Sign'\n",
        "config['DATASET']['H2S']['MEAN_PATH'] = f'{drive_data}/smpl-x/mean.pt'\n",
        "config['DATASET']['H2S']['STD_PATH'] = f'{drive_data}/smpl-x/std.pt'\n",
        "config['TRAIN']['PRETRAINED_VAE'] = f'{drive_data}/checkpoints/vae/tokenizer.ckpt'\n",
        "\n",
        "with open('configs/web_inference.yaml', 'w') as f:\n",
        "    yaml.dump(config, f)\n",
        "\n",
        "with open('configs/assets.yaml', 'r') as f:\n",
        "    assets = yaml.safe_load(f)\n",
        "\n",
        "assets['RENDER']['SMPL_MODEL_PATH'] = 'deps/smpl_models/smpl'\n",
        "assets['RENDER']['MODEL_PATH'] = 'deps/smpl_models'\n",
        "assets['METRIC']['TM2T']['t2m_path'] = f'{drive_data}/deps/deps/t2m/t2m/'\n",
        "\n",
        "with open('configs/assets_web.yaml', 'w') as f:\n",
        "    yaml.dump(assets, f)\n",
        "\n",
        "sys.argv = ['', '--cfg', 'configs/web_inference.yaml', '--cfg_assets', 'configs/assets_web.yaml']\n",
        "cfg = parse_args(phase=\"test\")\n",
        "cfg.FOLDER = cfg.TEST.FOLDER\n",
        "print(\"‚úì Configuration ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bs_hjT8bfabE",
        "outputId": "b7b2f965-82e7-45b0-b9a5-e2ed00b8d106",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightning_fabric.utilities.seed:Seed set to 1234\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean path /content/drive/MyDrive/GraduationProject/CodeFiles/SaSOKE/smpl-x/mean.pt std_path:  /content/drive/MyDrive/GraduationProject/CodeFiles/SaSOKE/smpl-x/std.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
            "INFO:root:Loading pretrain vae from /content/drive/MyDrive/GraduationProject/CodeFiles/SaSOKE/checkpoints/vae/tokenizer.ckpt\n",
            "2025-12-05 21:55:08,918 Loading pretrain vae from /content/drive/MyDrive/GraduationProject/CodeFiles/SaSOKE/checkpoints/vae/tokenizer.ckpt\n",
            "INFO:root:Loading pretrain model from /content/drive/MyDrive/GraduationProject/CodeFiles/SaSOKE/checkpoints/mGPT/last.ckpt\n",
            "2025-12-05 21:55:08,987 Loading pretrain model from /content/drive/MyDrive/GraduationProject/CodeFiles/SaSOKE/checkpoints/mGPT/last.ckpt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load rhand vae...\n",
            "\n",
            "=======Check Weights Loading======\n",
            "Weights not used from pretrained file:\n",
            "---------------------------\n",
            "Weights not loaded into new model:\n",
            "===================================\n",
            "\n",
            "load hand vae...\n",
            "\n",
            "=======Check Weights Loading======\n",
            "Weights not used from pretrained file:\n",
            "---------------------------\n",
            "Weights not loaded into new model:\n",
            "===================================\n",
            "\n",
            "load vae...\n",
            "\n",
            "=======Check Weights Loading======\n",
            "Weights not used from pretrained file:\n",
            "---------------------------\n",
            "Weights not loaded into new model:\n",
            "===================================\n",
            "\n",
            "\n",
            "‚úÖ Model loaded and ready!\n"
          ]
        }
      ],
      "source": [
        "# Load model\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "from mGPT.models.build_model import build_model\n",
        "from mGPT.data.build_data import build_data\n",
        "from mGPT.utils.load_checkpoint import load_pretrained_vae, load_pretrained\n",
        "from mGPT.utils.logger import create_logger\n",
        "from mGPT.utils.human_models import smpl_x, get_coord\n",
        "\n",
        "pl.seed_everything(cfg.SEED_VALUE)\n",
        "cfg.DATASET.WORD_VERTILIZER_PATH = f'{drive_data}/deps/deps/t2m/glove/'\n",
        "\n",
        "datamodule = build_data(cfg)\n",
        "model = build_model(cfg, datamodule)\n",
        "\n",
        "logger = create_logger(cfg, phase=\"test\")\n",
        "if cfg.TRAIN.PRETRAINED_VAE:\n",
        "    load_pretrained_vae(cfg, model, logger)\n",
        "\n",
        "ckpt_path = f'{drive_data}/checkpoints/mGPT/last.ckpt'\n",
        "if os.path.exists(ckpt_path):\n",
        "    cfg.TEST.CHECKPOINTS = ckpt_path\n",
        "    load_pretrained(cfg, model, logger, phase=\"test\")\n",
        "\n",
        "model = model.cuda()\n",
        "model.eval()\n",
        "\n",
        "mean = datamodule.hparams.mean.cuda()\n",
        "std = datamodule.hparams.std.cuda()\n",
        "\n",
        "print(\"\\n‚úÖ Model loaded and ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_U0TU2yfabE"
      },
      "source": [
        "## Step 3: Create API Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Yvppg99EfabF",
        "outputId": "7ab69f9c-afdc-4c7f-ef6c-0969c6fe0bd8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì API functions ready!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import json\n",
        "import trimesh\n",
        "import base64\n",
        "from io import BytesIO\n",
        "\n",
        "def feats_to_smplx_dict(features, mean_tensor, std_tensor):\n",
        "    \"\"\"Convert 133-dim features to SMPL-X parameters dictionary\"\"\"\n",
        "    features = features * std_tensor + mean_tensor\n",
        "    T = features.shape[0]\n",
        "    zero_pose = torch.zeros(T, 36).to(features)\n",
        "    features_full = torch.cat([zero_pose, features], dim=-1)  # (T, 169)\n",
        "\n",
        "    # Extract SMPL-X parameters as dictionary\n",
        "    smplx_params = {\n",
        "        'root_pose': features_full[:, 0:3].cpu().numpy().tolist(),\n",
        "        'body_pose': features_full[:, 3:66].cpu().numpy().tolist(),\n",
        "        'lhand_pose': features_full[:, 66:111].cpu().numpy().tolist(),\n",
        "        'rhand_pose': features_full[:, 111:156].cpu().numpy().tolist(),\n",
        "        'jaw_pose': features_full[:, 156:159].cpu().numpy().tolist(),\n",
        "        'expression': features_full[:, 159:169].cpu().numpy().tolist(),\n",
        "    }\n",
        "    return smplx_params\n",
        "\n",
        "def smplx_params_to_glb_frames(smplx_params_dict, num_frames):\n",
        "    \"\"\"Convert SMPL-X parameters to GLB frames (base64 encoded)\"\"\"\n",
        "    # Create shape parameter\n",
        "    shape_param = torch.tensor([[-0.07284723, 0.1795129, -0.27608207, 0.135155, 0.10748172,\n",
        "                                 0.16037364, -0.01616933, -0.03450319, 0.01369138, 0.01108842]],\n",
        "                               device=mean.device, dtype=torch.float32)\n",
        "\n",
        "    glb_frames = []\n",
        "\n",
        "    for i in range(num_frames):\n",
        "        # Convert lists back to tensors\n",
        "        root_pose = torch.tensor([smplx_params_dict['root_pose'][i]], dtype=torch.float32, device=mean.device)\n",
        "        body_pose = torch.tensor([smplx_params_dict['body_pose'][i]], dtype=torch.float32, device=mean.device)\n",
        "        lhand_pose = torch.tensor([smplx_params_dict['lhand_pose'][i]], dtype=torch.float32, device=mean.device)\n",
        "        rhand_pose = torch.tensor([smplx_params_dict['rhand_pose'][i]], dtype=torch.float32, device=mean.device)\n",
        "        jaw_pose = torch.tensor([smplx_params_dict['jaw_pose'][i]], dtype=torch.float32, device=mean.device)\n",
        "        expression = torch.tensor([smplx_params_dict['expression'][i]], dtype=torch.float32, device=mean.device)\n",
        "\n",
        "        # Generate mesh\n",
        "        with torch.no_grad():\n",
        "            vertices, _ = get_coord(\n",
        "                root_pose=root_pose,\n",
        "                body_pose=body_pose,\n",
        "                lhand_pose=lhand_pose,\n",
        "                rhand_pose=rhand_pose,\n",
        "                jaw_pose=jaw_pose,\n",
        "                shape=shape_param,\n",
        "                expr=expression\n",
        "            )\n",
        "\n",
        "        # Create trimesh with WHITE color\n",
        "        mesh = trimesh.Trimesh(\n",
        "            vertices=vertices[0].cpu().numpy(),\n",
        "            faces=smpl_x.face,\n",
        "            process=False\n",
        "        )\n",
        "        mesh.visual.vertex_colors = np.array([[255, 255, 255, 255]] * len(mesh.vertices))\n",
        "\n",
        "        # Export to GLB and encode to base64\n",
        "        glb_buffer = BytesIO()\n",
        "        mesh.export(file_obj=glb_buffer, file_type='glb')\n",
        "        glb_data = base64.b64encode(glb_buffer.getvalue()).decode('utf-8')\n",
        "        glb_frames.append(glb_data)\n",
        "\n",
        "    return glb_frames\n",
        "\n",
        "def generate_smplx_params(text, lang_token):\n",
        "    \"\"\"Generate SMPL-X parameters from text and language token\"\"\"\n",
        "    if not text.strip():\n",
        "        return None, \"‚ö†Ô∏è Please enter some text\"\n",
        "\n",
        "    try:\n",
        "        batch = {'text': [text], 'length': [0], 'src': [lang_token]}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model.forward(batch, task=\"t2m\")\n",
        "\n",
        "        feats = output['feats'][0] if 'feats' in output else None\n",
        "\n",
        "        if feats is None:\n",
        "            return None, \"‚ùå Generation failed - no features produced\"\n",
        "\n",
        "        smplx_params = feats_to_smplx_dict(feats, mean, std)\n",
        "\n",
        "        return smplx_params, None\n",
        "\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None, f\"‚ùå Error: {str(e)}\"\n",
        "\n",
        "print(\"‚úì API functions ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18IBgDOQfabG"
      },
      "source": [
        "## Step 4: Create Flask API Server"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5twRir2ifabG",
        "outputId": "e6de5fca-63f8-4292-8d2e-f9a198863510",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:40819\n",
            " * Running on http://172.28.0.12:40819\n",
            "2025-12-05 21:58:48,706 \u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:40819\n",
            " * Running on http://172.28.0.12:40819\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "2025-12-05 21:58:48,711 \u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Flask API started!\n",
            "üì° API URL: http://localhost:40819/api/generate\n",
            "üíö Health check: http://localhost:40819/api/health\n",
            "\n",
            "üìù API Usage:\n",
            "  POST /api/generate\n",
            "  Body: {'text': 'Hello world', 'lang_token': 'how2sign'}\n",
            "  Response: {'success': True, 'glb_frames': [...], 'num_frames': N}\n",
            "\n",
            "‚ö†Ô∏è  To expose this API publicly, use ngrok:\n",
            "  !pip install pyngrok\n",
            "  from pyngrok import ngrok\n",
            "  public_url = ngrok.connect(40819)\n",
            "  print(f'Public URL: {public_url}')\n"
          ]
        }
      ],
      "source": [
        "# Create Flask API\n",
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "from threading import Thread\n",
        "import socket\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app)  # Enable CORS for frontend access\n",
        "\n",
        "@app.route('/api/generate', methods=['POST'])\n",
        "def api_generate():\n",
        "    \"\"\"API endpoint that takes lang_token and text, returns GLB frames\"\"\"\n",
        "    try:\n",
        "        data = request.get_json()\n",
        "        text = data.get('text', '').strip()\n",
        "        lang_token = data.get('lang_token', 'how2sign')\n",
        "\n",
        "        if not text:\n",
        "            return jsonify({'error': 'Text is required'}), 400\n",
        "\n",
        "        # Generate SMPL-X parameters\n",
        "        smplx_params, error = generate_smplx_params(text, lang_token)\n",
        "\n",
        "        if error:\n",
        "            return jsonify({'error': error}), 500\n",
        "\n",
        "        num_frames = len(smplx_params['body_pose'])\n",
        "\n",
        "        # Convert to GLB frames\n",
        "        glb_frames = smplx_params_to_glb_frames(smplx_params, num_frames)\n",
        "\n",
        "        # Return GLB frames (base64 encoded)\n",
        "        return jsonify({\n",
        "            'success': True,\n",
        "            'glb_frames': glb_frames,\n",
        "            'num_frames': num_frames,\n",
        "            'text': text,\n",
        "            'lang_token': lang_token\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return jsonify({'error': str(e)}), 500\n",
        "\n",
        "@app.route('/api/health', methods=['GET'])\n",
        "def health():\n",
        "    \"\"\"Health check endpoint\"\"\"\n",
        "    return jsonify({'status': 'ok', 'message': 'API is running'})\n",
        "\n",
        "def get_free_port():\n",
        "    \"\"\"Get a free port\"\"\"\n",
        "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "        s.bind(('', 0))\n",
        "        s.listen(1)\n",
        "        port = s.getsockname()[1]\n",
        "    return port\n",
        "\n",
        "port = get_free_port()\n",
        "\n",
        "def run_flask():\n",
        "    app.run(host='0.0.0.0', port=port, debug=False, use_reloader=False)\n",
        "\n",
        "flask_thread = Thread(target=run_flask, daemon=True)\n",
        "flask_thread.start()\n",
        "\n",
        "# Wait a moment for Flask to start\n",
        "time.sleep(2)\n",
        "\n",
        "print(f\"\\n‚úÖ Flask API started!\")\n",
        "print(f\"üì° API URL: http://localhost:{port}/api/generate\")\n",
        "print(f\"üíö Health check: http://localhost:{port}/api/health\")\n",
        "print(f\"\\nüìù API Usage:\")\n",
        "print(f\"  POST /api/generate\")\n",
        "print(f\"  Body: {{'text': 'Hello world', 'lang_token': 'how2sign'}}\")\n",
        "print(f\"  Response: {{'success': True, 'glb_frames': [...], 'num_frames': N}}\")\n",
        "print(f\"\\n‚ö†Ô∏è  To expose this API publicly, use ngrok:\")\n",
        "print(f\"  !pip install pyngrok\")\n",
        "print(f\"  from pyngrok import ngrok\")\n",
        "print(f\"  public_url = ngrok.connect({port})\")\n",
        "print(f\"  print(f'Public URL: {{public_url}}')\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1fojggOfabH"
      },
      "source": [
        "## Step 5: Expose API Publicly (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gkKP_r4yfabH",
        "outputId": "633d4d26-3f8f-4ac1-8257-b919a470d7bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 693
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pyngrok.ngrok:Opening tunnel named: http-40819-09720348-4043-42c0-8f4f-1d128deea190\n",
            "2025-12-05 21:59:46,944 Opening tunnel named: http-40819-09720348-4043-42c0-8f4f-1d128deea190\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pyngrok.process.ngrok:t=2025-12-05T21:59:48+0000 lvl=info msg=\"no configuration paths supplied\"\n",
            "2025-12-05 21:59:48,312 t=2025-12-05T21:59:48+0000 lvl=info msg=\"no configuration paths supplied\"\n",
            "INFO:pyngrok.process.ngrok:t=2025-12-05T21:59:48+0000 lvl=info msg=\"using configuration at default config path\" path=/root/.config/ngrok/ngrok.yml\n",
            "2025-12-05 21:59:48,317 t=2025-12-05T21:59:48+0000 lvl=info msg=\"using configuration at default config path\" path=/root/.config/ngrok/ngrok.yml\n",
            "INFO:pyngrok.process.ngrok:t=2025-12-05T21:59:48+0000 lvl=info msg=\"open config file\" path=/root/.config/ngrok/ngrok.yml err=nil\n",
            "2025-12-05 21:59:48,323 t=2025-12-05T21:59:48+0000 lvl=info msg=\"open config file\" path=/root/.config/ngrok/ngrok.yml err=nil\n",
            "INFO:pyngrok.process.ngrok:t=2025-12-05T21:59:48+0000 lvl=info msg=\"FIPS 140 mode\" enabled=false\n",
            "2025-12-05 21:59:48,370 t=2025-12-05T21:59:48+0000 lvl=info msg=\"FIPS 140 mode\" enabled=false\n",
            "INFO:pyngrok.process.ngrok:t=2025-12-05T21:59:48+0000 lvl=info msg=\"starting web service\" obj=web addr=127.0.0.1:4040 allow_hosts=[]\n",
            "2025-12-05 21:59:48,385 t=2025-12-05T21:59:48+0000 lvl=info msg=\"starting web service\" obj=web addr=127.0.0.1:4040 allow_hosts=[]\n",
            "ERROR:pyngrok.process.ngrok:t=2025-12-05T21:59:48+0000 lvl=eror msg=\"failed to reconnect session\" obj=tunnels.session err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n",
            "2025-12-05 21:59:48,619 t=2025-12-05T21:59:48+0000 lvl=eror msg=\"failed to reconnect session\" obj=tunnels.session err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n",
            "ERROR:pyngrok.process.ngrok:t=2025-12-05T21:59:48+0000 lvl=eror msg=\"session closing\" obj=tunnels.session err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n",
            "2025-12-05 21:59:48,620 t=2025-12-05T21:59:48+0000 lvl=eror msg=\"session closing\" obj=tunnels.session err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "PyngrokNgrokError",
          "evalue": "The ngrok process errored on start: authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPyngrokNgrokError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3849958478.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Create public tunnel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mpublic_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngrok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nüåê Public API URL: {public_url}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nüìã Update your frontend with this URL:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(addr, proto, name, pyngrok_config, **options)\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Opening tunnel named: {name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m     \u001b[0mapi_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ngrok_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Creating tunnel with options: {options}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mget_ngrok_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0minstall_ngrok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyngrok/process.py\u001b[0m in \u001b[0;36mget_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_current_processes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngrok_path\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_start_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyngrok/process.py\u001b[0m in \u001b[0;36m_start_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mngrok_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartup_error\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m             raise PyngrokNgrokError(f\"The ngrok process errored on start: {ngrok_process.startup_error}.\",\n\u001b[0m\u001b[1;32m    448\u001b[0m                                     \u001b[0mngrok_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m                                     ngrok_process.startup_error)\n",
            "\u001b[0;31mPyngrokNgrokError\u001b[0m: The ngrok process errored on start: authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n."
          ]
        }
      ],
      "source": [
        "# Expose API using ngrok (for frontend access)\n",
        "!pip install -q pyngrok\n",
        "\n",
        "from pyngrok import ngrok\n",
        "\n",
        "\n",
        "# Create public tunnel\n",
        "public_url = ngrok.connect(port)\n",
        "print(f\"\\nüåê Public API URL: {public_url}\")\n",
        "print(f\"\\nüìã Update your frontend with this URL:\")\n",
        "print(f\"   const API_URL = '{public_url}/api/generate';\")\n",
        "print(f\"\\n‚ö†Ô∏è  This URL will expire when the Colab session ends!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xgIOATjghys1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}