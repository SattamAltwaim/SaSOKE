{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/SattamAltwaim/SaSOKE/blob/main/notebooks/9_api_server.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ü SOKE API Server\n",
    "\n",
    "**Flask API that provides SMPL-X parameters and GLB frames for sign language generation**\n",
    "\n",
    "### Features\n",
    "- ‚úÖ REST API endpoint for text-to-sign generation\n",
    "- ‚úÖ Returns GLB frames (base64 encoded) ready for 3D display\n",
    "- ‚úÖ CORS enabled for frontend access\n",
    "- ‚úÖ Works with standalone Apple-style frontend\n",
    "\n",
    "### Requirements\n",
    "- **GPU Runtime**: `Runtime ‚Üí Change runtime type ‚Üí GPU`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo and mount Drive\n",
    "import os\n",
    "if not os.path.exists('/content/SaSOKE'):\n",
    "    !git clone https://github.com/SattamAltwaim/SaSOKE.git\n",
    "%cd /content/SaSOKE\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "drive_data = '/content/drive/MyDrive/GraduationProject/CodeFiles/SaSOKE'\n",
    "print(\"‚úì Ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q pytorch_lightning torchmetrics omegaconf shortuuid transformers diffusers einops wandb rich matplotlib\n",
    "!pip install -q smplx h5py scikit-image spacy ftfy more-itertools natsort tensorboard sentencepiece\n",
    "!pip install -q flask flask-cors trimesh\n",
    "print(\"‚úì Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup paths\n",
    "import sys\n",
    "import yaml\n",
    "from mGPT.config import parse_args\n",
    "\n",
    "deps_links = {\n",
    "    'deps/smpl_models': f'{drive_data}/deps/smpl_models',\n",
    "    'deps/mbart-h2s-csl-phoenix': f'{drive_data}/deps/mbart-h2s-csl-phoenix',\n",
    "}\n",
    "\n",
    "for expected_path, actual_path in deps_links.items():\n",
    "    if not os.path.exists(expected_path):\n",
    "        os.makedirs(os.path.dirname(expected_path), exist_ok=True)\n",
    "        os.symlink(actual_path, expected_path)\n",
    "\n",
    "with open('configs/soke.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "config['ACCELERATOR'] = 'gpu'\n",
    "config['DEVICE'] = [0]\n",
    "config['DATASET']['H2S']['ROOT'] = f'{drive_data}/data/How2Sign'\n",
    "config['DATASET']['H2S']['MEAN_PATH'] = f'{drive_data}/smpl-x/mean.pt'\n",
    "config['DATASET']['H2S']['STD_PATH'] = f'{drive_data}/smpl-x/std.pt'\n",
    "config['TRAIN']['PRETRAINED_VAE'] = f'{drive_data}/checkpoints/vae/tokenizer.ckpt'\n",
    "\n",
    "with open('configs/web_inference.yaml', 'w') as f:\n",
    "    yaml.dump(config, f)\n",
    "\n",
    "with open('configs/assets.yaml', 'r') as f:\n",
    "    assets = yaml.safe_load(f)\n",
    "\n",
    "assets['RENDER']['SMPL_MODEL_PATH'] = 'deps/smpl_models/smpl'\n",
    "assets['RENDER']['MODEL_PATH'] = 'deps/smpl_models'\n",
    "assets['METRIC']['TM2T']['t2m_path'] = f'{drive_data}/deps/deps/t2m/t2m/'\n",
    "\n",
    "with open('configs/assets_web.yaml', 'w') as f:\n",
    "    yaml.dump(assets, f)\n",
    "\n",
    "sys.argv = ['', '--cfg', 'configs/web_inference.yaml', '--cfg_assets', 'configs/assets_web.yaml']\n",
    "cfg = parse_args(phase=\"test\")\n",
    "cfg.FOLDER = cfg.TEST.FOLDER\n",
    "print(\"‚úì Configuration ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from mGPT.models.build_model import build_model\n",
    "from mGPT.data.build_data import build_data\n",
    "from mGPT.utils.load_checkpoint import load_pretrained_vae, load_pretrained\n",
    "from mGPT.utils.logger import create_logger\n",
    "from mGPT.utils.human_models import smpl_x, get_coord\n",
    "\n",
    "pl.seed_everything(cfg.SEED_VALUE)\n",
    "cfg.DATASET.WORD_VERTILIZER_PATH = f'{drive_data}/deps/deps/t2m/glove/'\n",
    "\n",
    "datamodule = build_data(cfg)\n",
    "model = build_model(cfg, datamodule)\n",
    "\n",
    "logger = create_logger(cfg, phase=\"test\")\n",
    "if cfg.TRAIN.PRETRAINED_VAE:\n",
    "    load_pretrained_vae(cfg, model, logger)\n",
    "\n",
    "ckpt_path = f'{drive_data}/experiments/mgpt/SOKE/checkpoints/last.ckpt'\n",
    "if os.path.exists(ckpt_path):\n",
    "    cfg.TEST.CHECKPOINTS = ckpt_path\n",
    "    load_pretrained(cfg, model, logger, phase=\"test\")\n",
    "\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "\n",
    "mean = datamodule.hparams.mean.cuda()\n",
    "std = datamodule.hparams.std.cuda()\n",
    "\n",
    "print(\"\\n‚úÖ Model loaded and ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create API Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "import trimesh\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "def feats_to_smplx_dict(features, mean_tensor, std_tensor):\n",
    "    \"\"\"Convert 133-dim features to SMPL-X parameters dictionary\"\"\"\n",
    "    features = features * std_tensor + mean_tensor\n",
    "    T = features.shape[0]\n",
    "    zero_pose = torch.zeros(T, 36).to(features)\n",
    "    features_full = torch.cat([zero_pose, features], dim=-1)  # (T, 169)\n",
    "    \n",
    "    # Extract SMPL-X parameters as dictionary\n",
    "    smplx_params = {\n",
    "        'root_pose': features_full[:, 0:3].cpu().numpy().tolist(),\n",
    "        'body_pose': features_full[:, 3:66].cpu().numpy().tolist(),\n",
    "        'lhand_pose': features_full[:, 66:111].cpu().numpy().tolist(),\n",
    "        'rhand_pose': features_full[:, 111:156].cpu().numpy().tolist(),\n",
    "        'jaw_pose': features_full[:, 156:159].cpu().numpy().tolist(),\n",
    "        'expression': features_full[:, 159:169].cpu().numpy().tolist(),\n",
    "    }\n",
    "    return smplx_params\n",
    "\n",
    "def smplx_params_to_glb_frames(smplx_params_dict, num_frames):\n",
    "    \"\"\"Convert SMPL-X parameters to GLB frames (base64 encoded)\"\"\"\n",
    "    # Create shape parameter\n",
    "    shape_param = torch.tensor([[-0.07284723, 0.1795129, -0.27608207, 0.135155, 0.10748172,\n",
    "                                 0.16037364, -0.01616933, -0.03450319, 0.01369138, 0.01108842]],\n",
    "                               device=mean.device, dtype=torch.float32)\n",
    "    \n",
    "    glb_frames = []\n",
    "    \n",
    "    for i in range(num_frames):\n",
    "        # Convert lists back to tensors\n",
    "        root_pose = torch.tensor([smplx_params_dict['root_pose'][i]], dtype=torch.float32, device=mean.device)\n",
    "        body_pose = torch.tensor([smplx_params_dict['body_pose'][i]], dtype=torch.float32, device=mean.device)\n",
    "        lhand_pose = torch.tensor([smplx_params_dict['lhand_pose'][i]], dtype=torch.float32, device=mean.device)\n",
    "        rhand_pose = torch.tensor([smplx_params_dict['rhand_pose'][i]], dtype=torch.float32, device=mean.device)\n",
    "        jaw_pose = torch.tensor([smplx_params_dict['jaw_pose'][i]], dtype=torch.float32, device=mean.device)\n",
    "        expression = torch.tensor([smplx_params_dict['expression'][i]], dtype=torch.float32, device=mean.device)\n",
    "        \n",
    "        # Generate mesh\n",
    "        with torch.no_grad():\n",
    "            vertices, _ = get_coord(\n",
    "                root_pose=root_pose,\n",
    "                body_pose=body_pose,\n",
    "                lhand_pose=lhand_pose,\n",
    "                rhand_pose=rhand_pose,\n",
    "                jaw_pose=jaw_pose,\n",
    "                shape=shape_param,\n",
    "                expr=expression\n",
    "            )\n",
    "        \n",
    "        # Create trimesh with WHITE color\n",
    "        mesh = trimesh.Trimesh(\n",
    "            vertices=vertices[0].cpu().numpy(),\n",
    "            faces=smpl_x.face,\n",
    "            process=False\n",
    "        )\n",
    "        mesh.visual.vertex_colors = np.array([[255, 255, 255, 255]] * len(mesh.vertices))\n",
    "        \n",
    "        # Export to GLB and encode to base64\n",
    "        glb_buffer = BytesIO()\n",
    "        mesh.export(file_obj=glb_buffer, file_type='glb')\n",
    "        glb_data = base64.b64encode(glb_buffer.getvalue()).decode('utf-8')\n",
    "        glb_frames.append(glb_data)\n",
    "    \n",
    "    return glb_frames\n",
    "\n",
    "def generate_smplx_params(text, lang_token):\n",
    "    \"\"\"Generate SMPL-X parameters from text and language token\"\"\"\n",
    "    if not text.strip():\n",
    "        return None, \"‚ö†Ô∏è Please enter some text\"\n",
    "    \n",
    "    try:\n",
    "        batch = {'text': [text], 'length': [0], 'src': [lang_token]}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model.forward(batch, task=\"t2m\")\n",
    "        \n",
    "        feats = output['feats'][0] if 'feats' in output else None\n",
    "        \n",
    "        if feats is None:\n",
    "            return None, \"‚ùå Generation failed - no features produced\"\n",
    "        \n",
    "        smplx_params = feats_to_smplx_dict(feats, mean, std)\n",
    "        \n",
    "        return smplx_params, None\n",
    "        \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, f\"‚ùå Error: {str(e)}\"\n",
    "\n",
    "print(\"‚úì API functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create Flask API Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Flask API\n",
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "from threading import Thread\n",
    "import socket\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)  # Enable CORS for frontend access\n",
    "\n",
    "@app.route('/api/generate', methods=['POST'])\n",
    "def api_generate():\n",
    "    \"\"\"API endpoint that takes lang_token and text, returns GLB frames\"\"\"\n",
    "    try:\n",
    "        data = request.get_json()\n",
    "        text = data.get('text', '').strip()\n",
    "        lang_token = data.get('lang_token', 'how2sign')\n",
    "        \n",
    "        if not text:\n",
    "            return jsonify({'error': 'Text is required'}), 400\n",
    "        \n",
    "        # Generate SMPL-X parameters\n",
    "        smplx_params, error = generate_smplx_params(text, lang_token)\n",
    "        \n",
    "        if error:\n",
    "            return jsonify({'error': error}), 500\n",
    "        \n",
    "        num_frames = len(smplx_params['body_pose'])\n",
    "        \n",
    "        # Convert to GLB frames\n",
    "        glb_frames = smplx_params_to_glb_frames(smplx_params, num_frames)\n",
    "        \n",
    "        # Return GLB frames (base64 encoded)\n",
    "        return jsonify({\n",
    "            'success': True,\n",
    "            'glb_frames': glb_frames,\n",
    "            'num_frames': num_frames,\n",
    "            'text': text,\n",
    "            'lang_token': lang_token\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "@app.route('/api/health', methods=['GET'])\n",
    "def health():\n",
    "    \"\"\"Health check endpoint\"\"\"\n",
    "    return jsonify({'status': 'ok', 'message': 'API is running'})\n",
    "\n",
    "def get_free_port():\n",
    "    \"\"\"Get a free port\"\"\"\n",
    "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "        s.bind(('', 0))\n",
    "        s.listen(1)\n",
    "        port = s.getsockname()[1]\n",
    "    return port\n",
    "\n",
    "port = get_free_port()\n",
    "\n",
    "def run_flask():\n",
    "    app.run(host='0.0.0.0', port=port, debug=False, use_reloader=False)\n",
    "\n",
    "flask_thread = Thread(target=run_flask, daemon=True)\n",
    "flask_thread.start()\n",
    "\n",
    "# Wait a moment for Flask to start\n",
    "time.sleep(2)\n",
    "\n",
    "print(f\"\\n‚úÖ Flask API started!\")\n",
    "print(f\"üì° API URL: http://localhost:{port}/api/generate\")\n",
    "print(f\"üíö Health check: http://localhost:{port}/api/health\")\n",
    "print(f\"\\nüìù API Usage:\")\n",
    "print(f\"  POST /api/generate\")\n",
    "print(f\"  Body: {{'text': 'Hello world', 'lang_token': 'how2sign'}}\")\n",
    "print(f\"  Response: {{'success': True, 'glb_frames': [...], 'num_frames': N}}\")\n",
    "print(f\"\\n‚ö†Ô∏è  To expose this API publicly, use ngrok:\")\n",
    "print(f\"  !pip install pyngrok\")\n",
    "print(f\"  from pyngrok import ngrok\")\n",
    "print(f\"  public_url = ngrok.connect({port})\")\n",
    "print(f\"  print(f'Public URL: {{public_url}}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Expose API Publicly (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expose API using ngrok (for frontend access)\n",
    "!pip install -q pyngrok\n",
    "\n",
    "from pyngrok import ngrok\n",
    "\n",
    "# Create public tunnel\n",
    "public_url = ngrok.connect(port)\n",
    "print(f\"\\nüåê Public API URL: {public_url}\")\n",
    "print(f\"\\nüìã Update your frontend with this URL:\")\n",
    "print(f\"   const API_URL = '{public_url}/api/generate';\")\n",
    "print(f\"\\n‚ö†Ô∏è  This URL will expire when the Colab session ends!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
