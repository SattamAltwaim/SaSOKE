{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLwh8NKYxiCH"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SattamAltwaim/SaSOKE/blob/main/notebooks/5_text_to_sign_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V87ZQVyDxiCI"
      },
      "source": [
        "# Text-to-Sign Language Inference\n",
        "Generate sign language from custom text input using SOKE model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1krduhAxiCJ"
      },
      "source": [
        "## 1. Setup Environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyMTjkLrxiCJ",
        "outputId": "5887e1db-f3bd-45f5-b5bc-8f3e0f43aeb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'SaSOKE'...\n",
            "remote: Enumerating objects: 434, done.\u001b[K\n",
            "remote: Counting objects: 100% (434/434), done.\u001b[K\n",
            "remote: Compressing objects: 100% (316/316), done.\u001b[K\n",
            "remote: Total 434 (delta 167), reused 361 (delta 111), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (434/434), 2.46 MiB | 2.82 MiB/s, done.\n",
            "Resolving deltas: 100% (167/167), done.\n",
            "/content/SaSOKE\n",
            "Mounted at /content/drive\n",
            "✓ Code: /content/SaSOKE\n",
            "✓ Data: /content/drive/MyDrive/GraduationProject/CodeFiles/SaSOKE\n"
          ]
        }
      ],
      "source": [
        "# Clone repo if not present\n",
        "import os\n",
        "if not os.path.exists('/content/SaSOKE'):\n",
        "    !git clone https://github.com/SattamAltwaim/SaSOKE.git\n",
        "%cd /content/SaSOKE\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "drive_data = '/content/drive/MyDrive/GraduationProject/CodeFiles/SaSOKE'\n",
        "print(\"✓ Code:\", os.getcwd())\n",
        "print(\"✓ Data:\", drive_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-IXnxBlgxiCK",
        "outputId": "95eee123-b981-401a-dab0-3610b9bd56cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/831.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m831.6/831.6 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install dependencies (if needed)\n",
        "# Install dependencies\n",
        "%pip install -q pytorch_lightning torchmetrics omegaconf shortuuid transformers diffusers einops wandb rich matplotlib\n",
        "%pip install -q smplx h5py scikit-image spacy ftfy more-itertools natsort tensorboard sentencepiece\n",
        "%pip install -q gdown pandas\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Hd8Qpv-xiCL"
      },
      "source": [
        "## 2. Verify GPU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nV59LGyJxiCL",
        "outputId": "6e667eb2-f28c-4a73-d44c-f9ba95b532b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available: True\n",
            "GPU: NVIDIA A100-SXM4-40GB\n",
            "GPU Memory: 42.47 GB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"⚠️ No GPU detected! Go to Runtime → Change runtime type → GPU\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zs6xb8_AxiCL"
      },
      "source": [
        "## 3. Enter Your Custom Text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZptvkCnFxiCL",
        "outputId": "c99bcfec-e10a-43df-bdfe-a8e1c39b1f61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Target sign language: how2sign\n",
            "\n",
            "Input texts:\n",
            "1. Hello, how are you today?\n",
            "2. Thank you for your help.\n",
            "3. I am learning sign language.\n"
          ]
        }
      ],
      "source": [
        "# Enter your text here - you can modify this!\n",
        "custom_texts = [\n",
        "    \"Hello, how are you today?\",\n",
        "    \"Thank you for your help.\",\n",
        "    \"I am learning sign language.\"\n",
        "]\n",
        "\n",
        "# Choose sign language (default: American Sign Language)\n",
        "# Options: 'how2sign' (ASL), 'csl' (Chinese SL), 'phoenix' (German SL)\n",
        "sign_language = 'how2sign'  # Change this to generate different sign languages\n",
        "\n",
        "print(f\"Target sign language: {sign_language}\")\n",
        "print(\"\\nInput texts:\")\n",
        "for i, text in enumerate(custom_texts, 1):\n",
        "    print(f\"{i}. {text}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mf4GDaRm2Kug"
      },
      "source": [
        "### About the Three Sign Languages\n",
        "\n",
        "The SOKE model supports three sign languages:\n",
        "\n",
        "1. **`how2sign`** → American Sign Language (ASL)\n",
        "   - Uses language token `en_ASL`\n",
        "   - Input text: English\n",
        "   \n",
        "2. **`csl`** → Chinese Sign Language (CSL)\n",
        "   - Uses language token `zh_CSL`\n",
        "   - Input text: Chinese\n",
        "   \n",
        "3. **`phoenix`** → German Sign Language (DGS)\n",
        "   - Uses language token `de_DGS`\n",
        "   - Input text: German\n",
        "\n",
        "The model automatically converts your text to the appropriate sign language gestures based on your selection!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vyMiJL4xiCL"
      },
      "source": [
        "## 4. Run Inference on Your Text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for circular symlinks INSIDE t2m/t2m/ directory\n",
        "import os\n",
        "\n",
        "t2m_base = '/content/drive/MyDrive/GraduationProject/CodeFiles/SaSOKE/deps/t2m/t2m'\n",
        "\n",
        "print(f\"Contents of {t2m_base}:\")\n",
        "try:\n",
        "    for item in os.listdir(t2m_base):\n",
        "        item_path = os.path.join(t2m_base, item)\n",
        "        is_link = os.path.islink(item_path)\n",
        "        exists = os.path.exists(item_path)\n",
        "        \n",
        "        if is_link:\n",
        "            try:\n",
        "                target = os.readlink(item_path)\n",
        "                print(f\"  {item}: SYMLINK → {target}\")\n",
        "                \n",
        "                # Check if it's a circular symlink pointing back to t2m\n",
        "                if 't2m' in target and (target.endswith('t2m') or '/t2m/' in target):\n",
        "                    print(f\"    ⚠️ CIRCULAR SYMLINK DETECTED!\")\n",
        "                    print(f\"    Removing: {item_path}\")\n",
        "                    os.unlink(item_path)\n",
        "                    print(f\"    ✓ Removed\")\n",
        "            except Exception as e:\n",
        "                print(f\"  {item}: SYMLINK (broken or circular) - Error: {e}\")\n",
        "        else:\n",
        "            item_type = \"DIR\" if os.path.isdir(item_path) else \"FILE\"\n",
        "            print(f\"  {item}: {item_type}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error listing directory: {e}\")\n",
        "\n",
        "print(\"\\n✓ Check complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nb-vNa0Rq3QE",
        "outputId": "2af0f790-038f-4fdb-d1e1-0b7710092d7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating symbolic link from 'deps/smpl_models' to '/content/drive/MyDrive/GraduationProject/CodeFiles/SaSOKE/deps/smpl_models'\n",
            "  ✓ deps/smpl_models linked\n",
            "Creating symbolic link from 'deps/mbart-h2s-csl-phoenix' to '/content/drive/MyDrive/GraduationProject/CodeFiles/SaSOKE/deps/mbart-h2s-csl-phoenix'\n",
            "  ✓ deps/mbart-h2s-csl-phoenix linked\n",
            "\n",
            "✓ All symbolic links created!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Define the expected path and the actual path in Google Drive\n",
        "drive_data = '/content/drive/MyDrive/GraduationProject/CodeFiles/SaSOKE' # Ensure drive_data is defined\n",
        "\n",
        "# Create symbolic links for deps directories\n",
        "# Note: We don't symlink deps/t2m to avoid circular reference issues\n",
        "deps_links = {\n",
        "    'deps/smpl_models': f'{drive_data}/deps/smpl_models',\n",
        "    'deps/mbart-h2s-csl-phoenix': f'{drive_data}/deps/mbart-h2s-csl-phoenix',\n",
        "}\n",
        "\n",
        "for expected_path, actual_path in deps_links.items():\n",
        "    if not os.path.exists(expected_path):\n",
        "        print(f\"Creating symbolic link from '{expected_path}' to '{actual_path}'\")\n",
        "        # Ensure the parent directory for the symlink exists\n",
        "        os.makedirs(os.path.dirname(expected_path), exist_ok=True)\n",
        "        os.symlink(actual_path, expected_path)\n",
        "        print(f\"  ✓ {expected_path} linked\")\n",
        "    else:\n",
        "        print(f\"  ✓ {expected_path} already exists\")\n",
        "\n",
        "print(\"\\n✓ All symbolic links created!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab73372c",
        "outputId": "35df7abe-8156-41d9-d1ac-a92ad12bba36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Force no debugging when testing\n",
            "✓ Configuration and arguments parsed!\n"
          ]
        }
      ],
      "source": [
        "# Configuration and Argument Parsing (Run this cell ONLY ONCE per runtime session)\n",
        "import sys\n",
        "import yaml\n",
        "from omegaconf import OmegaConf\n",
        "from mGPT.config import parse_args\n",
        "\n",
        "# Configure paths\n",
        "with open('configs/soke.yaml', 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "config['ACCELERATOR'] = 'gpu'\n",
        "config['DEVICE'] = [0]\n",
        "config['DATASET']['H2S']['ROOT'] = f'{drive_data}/data/How2Sign'\n",
        "config['DATASET']['H2S']['MEAN_PATH'] = f'{drive_data}/smpl-x/mean.pt'\n",
        "config['DATASET']['H2S']['STD_PATH'] = f'{drive_data}/smpl-x/std.pt'\n",
        "config['TRAIN']['PRETRAINED_VAE'] = f'{drive_data}/checkpoints/vae/tokenizer.ckpt'\n",
        "\n",
        "# Disable metrics for inference (avoids loading text_mot_match which has circular symlink)\n",
        "config['METRIC'] = {'TYPE': []}\n",
        "\n",
        "with open('configs/text_inference.yaml', 'w') as f:\n",
        "    yaml.dump(config, f)\n",
        "\n",
        "# Update assets with the correct path from Google Drive\n",
        "with open('configs/assets.yaml', 'r') as f:\n",
        "    assets = yaml.safe_load(f)\n",
        "\n",
        "# Use mix of symlinks and full paths to avoid circular reference\n",
        "assets['RENDER']['SMPL_MODEL_PATH'] = 'deps/smpl_models/smpl'\n",
        "assets['RENDER']['MODEL_PATH'] = 'deps/smpl_models'\n",
        "# Use full Google Drive path for t2m to avoid circular symlink issues\n",
        "assets['METRIC']['TM2T']['t2m_path'] = f'{drive_data}/deps/t2m/t2m/'\n",
        "\n",
        "with open('configs/assets_inference.yaml', 'w') as f:\n",
        "    yaml.dump(assets, f)\n",
        "\n",
        "# Parse config (the parse_args function handles resolver registration)\n",
        "sys.argv = ['', '--cfg', 'configs/text_inference.yaml', '--cfg_assets', 'configs/assets_inference.yaml']\n",
        "\n",
        "# parse_args will register the 'eval' resolver automatically\n",
        "cfg = parse_args(phase=\"test\")\n",
        "cfg.FOLDER = cfg.TEST.FOLDER\n",
        "\n",
        "print(\"✓ Configuration and arguments parsed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "b539455d",
        "outputId": "45528ef9-655c-4aac-f7c2-bee4fbc126c4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:lightning_fabric.utilities.seed:Seed set to 1234\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model...\n",
            "Word vectorizer path being used: /content/drive/MyDrive/GraduationProject/CodeFiles/SaSOKE/deps/t2m/t2m/glove/\n",
            "mean path /content/drive/MyDrive/GraduationProject/CodeFiles/SaSOKE/smpl-x/mean.pt std_path:  /content/drive/MyDrive/GraduationProject/CodeFiles/SaSOKE/smpl-x/std.pt\n"
          ]
        },
        {
          "ename": "OSError",
          "evalue": "[Errno 40] Too many levels of symbolic links: '/content/drive/MyDrive/GraduationProject/CodeFiles/SaSOKE/deps/t2m/t2m/t2m/text_mot_match/model/finest.tar'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3770285183.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Word vectorizer path being used: {cfg.DATASET.WORD_VERTILIZER_PATH}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Added print statement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mdatamodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Load checkpoints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/SaSOKE/mGPT/models/build_model.py\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(cfg, datamodule)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmodel_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cfg'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmodel_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'datamodule'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0minstantiate_from_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/SaSOKE/mGPT/config.py\u001b[0m in \u001b[0;36minstantiate_from_config\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m\"target\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expected key `target` to instantiate.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_obj_from_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"params\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/SaSOKE/mGPT/models/mgpt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cfg, datamodule, lm, motion_vae, codebook_size, stage, debug, condition, task, metrics_dict, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'datamodule'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatamodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# Instantiate motion tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/SaSOKE/mGPT/models/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigure_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Ablation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/SaSOKE/mGPT/models/base.py\u001b[0m in \u001b[0;36mconfigure_metrics\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconfigure_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBaseMetrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatamodule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_npy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/SaSOKE/mGPT/metrics/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cfg, datamodule, debug, **kwargs)\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0mdiversity_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdebug\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMETRIC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDIVERSITY_TIMES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 dist_sync_on_step=cfg.METRIC.DIST_SYNC_ON_STEP)\n\u001b[0;32m---> 29\u001b[0;31m             self.MMMetrics = MMMetrics(\n\u001b[0m\u001b[1;32m     30\u001b[0m                 \u001b[0mcfg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0mmm_num_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMETRIC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMM_NUM_TIMES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/SaSOKE/mGPT/metrics/mm.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cfg, dataname, mm_num_times, dist_sync_on_step, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# T2M Evaluator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_t2m_evaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_t2m_evaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/SaSOKE/mGPT/metrics/mm.py\u001b[0m in \u001b[0;36m_get_t2m_evaluator\u001b[0;34m(self, cfg)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mdataname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"t2m\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         t2m_checkpoint = torch.load(os.path.join(\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMETRIC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTM2T\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt2m_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \"text_mot_match/model/finest.tar\"),\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1484\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1485\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFileLike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 40] Too many levels of symbolic links: 'deps/t2m/t2m/t2m/text_mot_match/model/finest.tar'"
          ]
        }
      ],
      "source": [
        "# Model Setup and Loading (You can rerun this cell as needed)\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "from mGPT.models.build_model import build_model\n",
        "from mGPT.data.build_data import build_data\n",
        "from mGPT.utils.load_checkpoint import load_pretrained_vae, load_pretrained\n",
        "from mGPT.utils.logger import create_logger\n",
        "import os\n",
        "\n",
        "# Assuming 'cfg' and 'drive_data' are defined from the previous cell\n",
        "\n",
        "# Seed\n",
        "pl.seed_everything(cfg.SEED_VALUE)\n",
        "\n",
        "# Update the word vectorizer path in cfg (word vectorizer is in glove/ subdirectory)\n",
        "# Use full Google Drive path\n",
        "cfg.DATASET.WORD_VERTILIZER_PATH = f'{drive_data}/deps/t2m/t2m/glove/'\n",
        "\n",
        "# Build data and model\n",
        "print(\"Loading model...\")\n",
        "print(f\"Word vectorizer path being used: {cfg.DATASET.WORD_VERTILIZER_PATH}\") # Added print statement\n",
        "datamodule = build_data(cfg)\n",
        "model = build_model(cfg, datamodule)\n",
        "\n",
        "# Load checkpoints\n",
        "logger = create_logger(cfg, phase=\"test\")\n",
        "if cfg.TRAIN.PRETRAINED_VAE:\n",
        "    load_pretrained_vae(cfg, model, logger)\n",
        "\n",
        "# Check for trained checkpoint\n",
        "ckpt_path = f'{drive_data}/experiments/mgpt/SOKE/checkpoints/last.ckpt'\n",
        "if os.path.exists(ckpt_path):\n",
        "    print(f\"Loading trained checkpoint from {ckpt_path}\")\n",
        "    cfg.TEST.CHECKPOINTS = ckpt_path\n",
        "    load_pretrained(cfg, model, logger, phase=\"test\")\n",
        "else:\n",
        "    print(\"Using pretrained mBART (no fine-tuned checkpoint found)\")\n",
        "\n",
        "model = model.cuda()\n",
        "model.eval()\n",
        "\n",
        "print(\"✓ Model ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "noXFgzhSxiCM"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Helper function to convert features to SMPL-X parameters\n",
        "def feats_to_smplx(features, mean, std):\n",
        "    \"\"\"Convert 133-dim compressed features to SMPL-X parameters.\"\"\"\n",
        "    # Denormalize features\n",
        "    features = features * std + mean\n",
        "\n",
        "    # Add zero root pose (36 dims) to get 169 dims total\n",
        "    T = features.shape[0]\n",
        "    zero_pose = torch.zeros(T, 36).to(features)\n",
        "    features_full = torch.cat([zero_pose, features], dim=-1)  # (T, 169)\n",
        "\n",
        "    # Extract SMPL-X parameters\n",
        "    smplx_params = {\n",
        "        'root_pose': features_full[:, 0:3].cpu().numpy(),\n",
        "        'body_pose': features_full[:, 3:66].cpu().numpy(),\n",
        "        'lhand_pose': features_full[:, 66:111].cpu().numpy(),\n",
        "        'rhand_pose': features_full[:, 111:156].cpu().numpy(),\n",
        "        'jaw_pose': features_full[:, 156:159].cpu().numpy(),\n",
        "        'expression': features_full[:, 159:169].cpu().numpy(),\n",
        "    }\n",
        "    return smplx_params\n",
        "\n",
        "# Generate sign language poses\n",
        "output_dir = '/content/text_sign_results'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "print(f\"\\nGenerating sign language for {len(custom_texts)} text(s)....\\n\")\n",
        "\n",
        "# Get mean and std for denormalization\n",
        "mean = datamodule.hparams.mean.cuda()\n",
        "std = datamodule.hparams.std.cuda()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for idx, text in enumerate(custom_texts):\n",
        "        print(f\"[{idx+1}/{len(custom_texts)}] Processing: '{text}'\")\n",
        "\n",
        "        # Prepare input (model expects text, length, and src fields)\n",
        "        batch = {\n",
        "            'text': [text],\n",
        "            'length': [0],  # length not used during generation\n",
        "            'src': [sign_language]  # Target sign language: 'how2sign', 'csl', or 'phoenix'\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Generate FULL SEQUENCE using forward method\n",
        "            output = model.forward(batch, task=\"t2m\")\n",
        "\n",
        "            # Extract features\n",
        "            feats = output['feats'][0] if 'feats' in output else None\n",
        "\n",
        "            if feats is None:\n",
        "                print(f\"  ✗ No features generated\")\n",
        "                continue\n",
        "\n",
        "            # Convert to SMPL-X parameters (full sequence)\n",
        "            smplx_params = feats_to_smplx(feats, mean, std)\n",
        "\n",
        "            # Save result (NO TOKENS, only SMPL-X params)\n",
        "            filename = f\"text_{idx+1}.pkl\"\n",
        "            filepath = os.path.join(output_dir, filename)\n",
        "\n",
        "            result = {\n",
        "                'text': text,\n",
        "                'smplx_params': smplx_params,  # Full sequence of SMPL-X poses\n",
        "                'num_frames': smplx_params['body_pose'].shape[0]\n",
        "            }\n",
        "\n",
        "            with open(filepath, 'wb') as f:\n",
        "                pickle.dump(result, f)\n",
        "\n",
        "            print(f\"  ✓ Saved: {filepath}\")\n",
        "            print(f\"    - Frames: {result['num_frames']}\")\n",
        "            print(f\"    - SMPL-X parameters saved (no tokens)\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ✗ Error: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            continue\n",
        "\n",
        "print(f\"\\nComplete! Predictions saved in '{output_dir}'\")\n",
        "print(f\"\\nTo play the animations, download results and use:\")\n",
        "print(f\"  python3 generate_animation_html.py text_sign_results/text_1.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBsZI_6rxiCM"
      },
      "source": [
        "## 5. View Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rd8ntE1xiCN"
      },
      "outputs": [],
      "source": [
        "# List generated files\n",
        "print(\"Generated predictions:\")\n",
        "!ls -lh {output_dir}\n",
        "\n",
        "# Load and display results\n",
        "pkl_files = sorted([f for f in os.listdir(output_dir) if f.endswith('.pkl')])\n",
        "\n",
        "for pkl_file in pkl_files:\n",
        "    filepath = os.path.join(output_dir, pkl_file)\n",
        "\n",
        "    with open(filepath, 'rb') as f:\n",
        "        result = pickle.load(f)\n",
        "\n",
        "    print(f\"\\n{pkl_file}:\")\n",
        "    print(f\"  Text: {result['text']}\")\n",
        "    print(f\"  Frames: {result['num_frames']}\")\n",
        "\n",
        "    # Display SMPL-X parameters info\n",
        "    if result.get('smplx_params') is not None:\n",
        "        smplx = result['smplx_params']\n",
        "        print(f\"  SMPL-X Parameters:\")\n",
        "        print(f\"- root_pose: {smplx['root_pose'].shape} (global orientation)\")\n",
        "        print(f\"- body_pose: {smplx['body_pose'].shape} (21 body joints × 3)\")\n",
        "        print(f\"- lhand_pose: {smplx['lhand_pose'].shape} (15 left hand joints × 3)\")\n",
        "        print(f\"- rhand_pose: {smplx['rhand_pose'].shape} (15 right hand joints × 3)\")\n",
        "        print(f\"- jaw_pose: {smplx['jaw_pose'].shape} (jaw rotation)\")\n",
        "        print(f\"- expression: {smplx['expression'].shape} (facial expression)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpZ7OBNcxiCN"
      },
      "source": [
        "## 6. Download Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gf7XrbBIxiCN"
      },
      "outputs": [],
      "source": [
        "# Zip results for easy download\n",
        "!zip -r text_sign_results.zip {output_dir}/\n",
        "\n",
        "# Download\n",
        "from google.colab import files\n",
        "files.download('text_sign_results.zip')\n",
        "\n",
        "print(\"✓ Results packaged and ready to download\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVdSirKUxiCN"
      },
      "source": [
        "## Notes\n",
        "\n",
        "- **GPU Required**: Make sure you're using a GPU runtime (Runtime → Change runtime type → GPU → T4/V100/A100)\n",
        "- **First Time**: Run notebook 1 first to download all dependencies to your Google Drive\n",
        "- **Custom Text**: Simply modify the `custom_texts` list in cell 8 with your own text\n",
        "- **Output**: Each text generates a `.pkl` file containing predicted sign language poses (3D coordinates)\n",
        "- **Format**: Poses are in SMPL-X format and can be visualized using 3D animation tools\n",
        "\n",
        "### Troubleshooting\n",
        "- **OOM Error**: Reduce text length or batch size\n",
        "- **Missing files**: Make sure notebook 1 was run successfully to download models\n",
        "- **Slow generation**: Normal on T4 GPU, faster on V100/A100\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
